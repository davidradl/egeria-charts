{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"template: home.html title: About License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"About"},{"location":"charts/base/","text":"Base Egeria (egeria-base) \u00b6 This is a simple deployment of Egeria which will just deploy a basic Egeria environment which is ready for you to experiment with. Specifically it sets up A single egeria platform which hosts An egeria metadata server with a persistent graph store A new server to support the UI A UI to allow browsing of types, instances & servers Apache Kafka & Zookeeper It does not provide access to our lab notebooks, the Polymer based UI, nor is it preloaded with any data. This chart may also be useful to understand how to deploy Egeria within kubernetes. In future we anticipate providing an operator which will be more flexible Prerequisites \u00b6 In order to use the labs, you'll first need to have the following installed: A Kubernetes cluster at 1.15 or above the kubectl tool in your path Helm 3.0 or above No configuration of the chart is required to use defaults, but information is provided below Installation \u00b6 From one directory level above the location of this README, run the following: helm dep update egeria-base helm install egeria egeria-base THE INSTALL WILL TAKE SEVERAL MINUTES This is because it is not only creating the required objects in Kubernetes to run the platforms, but also is configuring egeria itself - which involves waiting for everything to startup before configuring Egeria via REST API calls. Once installed the configured server is set to start automatically, storage is persisted, and so if your pod gets moved/restarted, egeria should come back automatically with the same data as before. Additional Install Configuration \u00b6 This section is optional - skip over if you're happy with defaults - a good idea to begin with. In a helm chart the configuration that has been externalised by the chart writer is specified in the values.yaml file which you can find in this directory. However rather than edit this file directly, it's recommended you create an additional file with the required overrides. As an example, in values.yaml we see a value 'serverName' which is set to mds1. If I want to override this I could do helm install --set-string egeria.serverName = myserver However this can get tedious with multiple values to override, and you need to know the correct types to use. Instead it may be easier to create an additional file. For example let's create a file in my home directory ~/egeria.yaml containing: egeria: serverName: metadataserver viewServerName: presentationview We can then install the chart with: helm install -f ~/egeria.yaml egeria egeria-base Up to now the installation has been called egeria but we could call it something else ie metadataserver helm install -f ~/egeria.yaml metadataserver egeria-base This is known as a release in Helm, and we can have multiple installed currently. To list these use helm list and to delete both names we've experimented with so far: helm delete metadataserver helm delete egeria Refer to the comments in values.yaml for further information on what can be configured - this includes: - server, organization, cohort names - storage options - k8s storage class/size - Egeria version - Kubernetes service setup (see below also) - roles & accounts - timeouts - names & repositories for docker images used by the chart - Kafka specific configuration (setup in the Bitnami chart we use) Using additional connectors \u00b6 If you have additional Egeria connectors that are needed in your deployment, you should soon be able to include the following when deploying the chart helm install --include-dir libs = ~/libs egeria egeria-base Where in this example ~/libs is the directory including the additional connectors you wish to use. However the support for this is awaiting a helm PR , so in the meantime please copy files directly into a 'libs' directory within the chart instead. For example mkdir egeria-base/libs cp ~/libs/*jar egeria-base/libs These files will be copied into a kubernetes config map, which is then made available as a mounted volume to the runtime image of egeria & added to the class loading path as /extlib . You still need to configure the egeria server(s) appropriately Accessing Egeria \u00b6 When this chart is installed, an initialization job is run to configure the egeria metadata server and UI. For example looking at kubernetes jobs will show something like: $ kubectl get pods [ 17 :27:11 ] NAME READY STATUS RESTARTS AGE egeria-base-config-crhrv 1 /1 Running 0 4m16s egeria-base-platform-0 1 /1 Running 0 4m16s egeria-base-presentation-699669cfd4-9swjb 1 /1 Running 0 4m16s egeria-kafka-0 1 /1 Running 2 4m16s egeria-zookeeper-0 1 /1 Running 0 4m16s You should wait until that first 'egeria-base-config' job completes, it will then disappear from the list. This job issues REST calls against the egeria serves to configure them for this simple environment (see scripts directory). The script will not run again, since we will have now configured the servers with persistent storage, and for the platform to autostart our servers. So even if a pod is removed and restarted, the egeria platform and servers should return in the same state. We now have egeria running within a Kubernetes cluster, but by default no services are exposed externally - they are all of type ClusterIP - we can see these with $ kubectl get services [ 12 :38:16 ] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE egeria-kafka ClusterIP 172 .21.42.105 <none> 9092 /TCP 33m egeria-kafka-headless ClusterIP None <none> 9092 /TCP,9093/TCP 33m egeria-platform ClusterIP 172 .21.40.79 <none> 9443 /TCP 33m egeria-presentation ClusterIP 172 .21.107.242 <none> 8091 /TCP 33m egeria-zookeeper ClusterIP 172 .21.126.56 <none> 2181 /TCP,2888/TCP,3888/TCP 33m egeria-zookeeper-headless ClusterIP None <none> 2181 /TCP,2888/TCP,3888/TCP 33m The egeria-presentation service is very useful to expose as this provides a useful UI where you can explore types and instances. Also egeria-platform is the service for egeria itself. In production you might want this only exposed very carefully to other systems - and not other users or the internet, but for experimenting with egeria let's assume you do. How these are exposed can be somewhat dependent on the specific kubernetes environment you are using. In the lab chart we provided an example of using kubectl port-forward . Here we use RedHat OpenShift in IBM Cloud, where you can expose these services via a LoadBalancer using kubectl expose service/egeria-presentation --type=LoadBalancer --port=8091 --target-port=8091 --name pres kubectl expose service/egeria-platform --type=LoadBalancer --port=9443 --target-port=9443 --name platform If I run these, and then look again at my services I see I now have 2 additional entries (modified for obfuscation): platform LoadBalancer 172.21.218.241 3bc644c3-eu-gb.lb.appdomain.cloud 9443:30640/TCP 25h pres LoadBalancer 172.21.18.10 42b0c7f5-eu-gb.lb.appdomain.cloud 8091:30311/TCP 22h So I can access them at their respective hosts. Note that where hosts are allocated dynamically, it can take up to an hour or more for DNS caches to refresh. Waiting up to 5 minutes then refreshing a local cache has proven sufficient for me, but your experience may differ. In the example above, the Egeria UI can be accessed at https://42b0c7f5-eu-gb.lb.appdomain.cloud:8091/org/ . Replace the hostname accordingly, and also the org is the organization name from the Values.yaml file we referred to above Once logged in, you should be able to login using our demo userid/password of garygeeke/admin & start browsing types and instances within Egeria. You can also issue REST API calls against egeria using a base URL for the platform of https://3bc644c3-eu-gb.lb.appdomain.cloud - Other material covers these REST API calls in more detail, but a simple api doc is available at https://3bc644c3-eu-gb.lb.appdomain.cloud/swagger-ui.html Cleaning up / removing Egeria \u00b6 To delete the deployment, simply run this for Helm3: $ helm delete egeria In addition, the egeria chart makes use of persistent storage. To remove these use $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-egeria-kafka-0 Bound pvc-d31e0012-8568-4e6f-ae5d-94832ebcd92d 10Gi RWO ibmc-vpc-block-10iops-tier 48m data-egeria-zookeeper-0 Bound pvc-90739fce-dce0-422b-8738-a38293d8fdfb 10Gi RWO ibmc-vpc-block-10iops-tier 48m egeria-egeria-data-egeria-base-platform-0 Bound pvc-197ba47e-a6d5-4f35-a7d8-7b7ec1ed1df3 10Gi RWO ibmc-vpc-block-10iops-tier 48m Identify those associated with egeria - which should be obvious from the name and then delete with kubectl delete pvc <id> See the section on Configuration for more details License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Base"},{"location":"charts/base/#base-egeria-egeria-base","text":"This is a simple deployment of Egeria which will just deploy a basic Egeria environment which is ready for you to experiment with. Specifically it sets up A single egeria platform which hosts An egeria metadata server with a persistent graph store A new server to support the UI A UI to allow browsing of types, instances & servers Apache Kafka & Zookeeper It does not provide access to our lab notebooks, the Polymer based UI, nor is it preloaded with any data. This chart may also be useful to understand how to deploy Egeria within kubernetes. In future we anticipate providing an operator which will be more flexible","title":"Base Egeria (egeria-base)"},{"location":"charts/base/#prerequisites","text":"In order to use the labs, you'll first need to have the following installed: A Kubernetes cluster at 1.15 or above the kubectl tool in your path Helm 3.0 or above No configuration of the chart is required to use defaults, but information is provided below","title":"Prerequisites"},{"location":"charts/base/#installation","text":"From one directory level above the location of this README, run the following: helm dep update egeria-base helm install egeria egeria-base THE INSTALL WILL TAKE SEVERAL MINUTES This is because it is not only creating the required objects in Kubernetes to run the platforms, but also is configuring egeria itself - which involves waiting for everything to startup before configuring Egeria via REST API calls. Once installed the configured server is set to start automatically, storage is persisted, and so if your pod gets moved/restarted, egeria should come back automatically with the same data as before.","title":"Installation"},{"location":"charts/base/#additional-install-configuration","text":"This section is optional - skip over if you're happy with defaults - a good idea to begin with. In a helm chart the configuration that has been externalised by the chart writer is specified in the values.yaml file which you can find in this directory. However rather than edit this file directly, it's recommended you create an additional file with the required overrides. As an example, in values.yaml we see a value 'serverName' which is set to mds1. If I want to override this I could do helm install --set-string egeria.serverName = myserver However this can get tedious with multiple values to override, and you need to know the correct types to use. Instead it may be easier to create an additional file. For example let's create a file in my home directory ~/egeria.yaml containing: egeria: serverName: metadataserver viewServerName: presentationview We can then install the chart with: helm install -f ~/egeria.yaml egeria egeria-base Up to now the installation has been called egeria but we could call it something else ie metadataserver helm install -f ~/egeria.yaml metadataserver egeria-base This is known as a release in Helm, and we can have multiple installed currently. To list these use helm list and to delete both names we've experimented with so far: helm delete metadataserver helm delete egeria Refer to the comments in values.yaml for further information on what can be configured - this includes: - server, organization, cohort names - storage options - k8s storage class/size - Egeria version - Kubernetes service setup (see below also) - roles & accounts - timeouts - names & repositories for docker images used by the chart - Kafka specific configuration (setup in the Bitnami chart we use)","title":"Additional Install Configuration"},{"location":"charts/base/#using-additional-connectors","text":"If you have additional Egeria connectors that are needed in your deployment, you should soon be able to include the following when deploying the chart helm install --include-dir libs = ~/libs egeria egeria-base Where in this example ~/libs is the directory including the additional connectors you wish to use. However the support for this is awaiting a helm PR , so in the meantime please copy files directly into a 'libs' directory within the chart instead. For example mkdir egeria-base/libs cp ~/libs/*jar egeria-base/libs These files will be copied into a kubernetes config map, which is then made available as a mounted volume to the runtime image of egeria & added to the class loading path as /extlib . You still need to configure the egeria server(s) appropriately","title":"Using additional connectors"},{"location":"charts/base/#accessing-egeria","text":"When this chart is installed, an initialization job is run to configure the egeria metadata server and UI. For example looking at kubernetes jobs will show something like: $ kubectl get pods [ 17 :27:11 ] NAME READY STATUS RESTARTS AGE egeria-base-config-crhrv 1 /1 Running 0 4m16s egeria-base-platform-0 1 /1 Running 0 4m16s egeria-base-presentation-699669cfd4-9swjb 1 /1 Running 0 4m16s egeria-kafka-0 1 /1 Running 2 4m16s egeria-zookeeper-0 1 /1 Running 0 4m16s You should wait until that first 'egeria-base-config' job completes, it will then disappear from the list. This job issues REST calls against the egeria serves to configure them for this simple environment (see scripts directory). The script will not run again, since we will have now configured the servers with persistent storage, and for the platform to autostart our servers. So even if a pod is removed and restarted, the egeria platform and servers should return in the same state. We now have egeria running within a Kubernetes cluster, but by default no services are exposed externally - they are all of type ClusterIP - we can see these with $ kubectl get services [ 12 :38:16 ] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE egeria-kafka ClusterIP 172 .21.42.105 <none> 9092 /TCP 33m egeria-kafka-headless ClusterIP None <none> 9092 /TCP,9093/TCP 33m egeria-platform ClusterIP 172 .21.40.79 <none> 9443 /TCP 33m egeria-presentation ClusterIP 172 .21.107.242 <none> 8091 /TCP 33m egeria-zookeeper ClusterIP 172 .21.126.56 <none> 2181 /TCP,2888/TCP,3888/TCP 33m egeria-zookeeper-headless ClusterIP None <none> 2181 /TCP,2888/TCP,3888/TCP 33m The egeria-presentation service is very useful to expose as this provides a useful UI where you can explore types and instances. Also egeria-platform is the service for egeria itself. In production you might want this only exposed very carefully to other systems - and not other users or the internet, but for experimenting with egeria let's assume you do. How these are exposed can be somewhat dependent on the specific kubernetes environment you are using. In the lab chart we provided an example of using kubectl port-forward . Here we use RedHat OpenShift in IBM Cloud, where you can expose these services via a LoadBalancer using kubectl expose service/egeria-presentation --type=LoadBalancer --port=8091 --target-port=8091 --name pres kubectl expose service/egeria-platform --type=LoadBalancer --port=9443 --target-port=9443 --name platform If I run these, and then look again at my services I see I now have 2 additional entries (modified for obfuscation): platform LoadBalancer 172.21.218.241 3bc644c3-eu-gb.lb.appdomain.cloud 9443:30640/TCP 25h pres LoadBalancer 172.21.18.10 42b0c7f5-eu-gb.lb.appdomain.cloud 8091:30311/TCP 22h So I can access them at their respective hosts. Note that where hosts are allocated dynamically, it can take up to an hour or more for DNS caches to refresh. Waiting up to 5 minutes then refreshing a local cache has proven sufficient for me, but your experience may differ. In the example above, the Egeria UI can be accessed at https://42b0c7f5-eu-gb.lb.appdomain.cloud:8091/org/ . Replace the hostname accordingly, and also the org is the organization name from the Values.yaml file we referred to above Once logged in, you should be able to login using our demo userid/password of garygeeke/admin & start browsing types and instances within Egeria. You can also issue REST API calls against egeria using a base URL for the platform of https://3bc644c3-eu-gb.lb.appdomain.cloud - Other material covers these REST API calls in more detail, but a simple api doc is available at https://3bc644c3-eu-gb.lb.appdomain.cloud/swagger-ui.html","title":"Accessing Egeria"},{"location":"charts/base/#cleaning-up-removing-egeria","text":"To delete the deployment, simply run this for Helm3: $ helm delete egeria In addition, the egeria chart makes use of persistent storage. To remove these use $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-egeria-kafka-0 Bound pvc-d31e0012-8568-4e6f-ae5d-94832ebcd92d 10Gi RWO ibmc-vpc-block-10iops-tier 48m data-egeria-zookeeper-0 Bound pvc-90739fce-dce0-422b-8738-a38293d8fdfb 10Gi RWO ibmc-vpc-block-10iops-tier 48m egeria-egeria-data-egeria-base-platform-0 Bound pvc-197ba47e-a6d5-4f35-a7d8-7b7ec1ed1df3 10Gi RWO ibmc-vpc-block-10iops-tier 48m Identify those associated with egeria - which should be obvious from the name and then delete with kubectl delete pvc <id> See the section on Configuration for more details License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Cleaning up / removing Egeria"},{"location":"charts/lab/","text":"Lab - Coco Pharmaceuticals (odpi-egeria-lab) \u00b6 This example is intended to replicate the metadata landscape of a hypothetical company, Coco Pharmaceuticals, and allow you to understand a little more about how Egeria can help, and how to use it. This forms our 'Hands on Lab'. The Helm chart will deploy the following components, all in a self-contained environment, allowing you to explore Egeria and its concepts safely and repeatably: Multiple Egeria servers Apache Kafka (and its Zookeeper dependency) Example Jupyter Notebooks Jupyter runtime Egeria's React based UI Prerequisites \u00b6 In order to use the labs, you'll first need to have the following installed: Kubernetes 1.15 or above Helm 3.0 or above Egeria chart repository configured & updated 6GB RAM minimum is recommended for your k8s environment. You no longer need a git clone of this repository to install the chart. Installation \u00b6 helm install lab egeria/odpi-egeria-lab In the following examples we're using microk8s. $ helm install lab egeria/odpi-egeria-lab [ 11 :47:04 ] WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /var/snap/microk8s/2346/credentials/client.config NAME: lab LAST DEPLOYED: Tue Aug 10 11 :47:19 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ODPi Egeria lab tutorial --- Some additional help text is also output, which is truncated for brevity. It can take a few seconds for the various components to all spin-up. You can monitor the readiness by running kubectl get all -- when ready, you should see output like the following: $ kubectl get all [ 13 :53:43 ] NAME READY STATUS RESTARTS AGE pod/lab-odpi-egeria-lab-ui-74cc464575-cf8rm 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-datalake-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-nginx-7b96949b4f-7dff4 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-presentation-bd9789747-rbv69 1 /1 Running 0 126m pod/lab-kafka-0 1 /1 Running 0 126m pod/lab-zookeeper-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-dev-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-uistatic-7b98d4bf9b-sf9bj 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-core-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-factory-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-jupyter-77b6868c4-9hnlp 1 /1 Running 0 126m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .152.183.1 <none> 443 /TCP 20h service/lab-kafka-headless ClusterIP None <none> 9092 /TCP,9093/TCP 126m service/lab-zookeeper-headless ClusterIP None <none> 2181 /TCP,2888/TCP,3888/TCP 126m service/lab-zookeeper ClusterIP 10 .152.183.25 <none> 2181 /TCP,2888/TCP,3888/TCP 126m service/lab-jupyter ClusterIP 10 .152.183.172 <none> 8888 /TCP 126m service/lab-core ClusterIP 10 .152.183.248 <none> 9443 /TCP 126m service/lab-nginx ClusterIP 10 .152.183.155 <none> 443 /TCP 126m service/lab-datalake ClusterIP 10 .152.183.191 <none> 9443 /TCP 126m service/lab-dev ClusterIP 10 .152.183.122 <none> 9443 /TCP 126m service/lab-kafka ClusterIP 10 .152.183.79 <none> 9092 /TCP 126m service/lab-odpi-egeria-lab-factory ClusterIP 10 .152.183.158 <none> 9443 /TCP 126m service/lab-uistatic ClusterIP 10 .152.183.156 <none> 80 /TCP 126m service/lab-presentation ClusterIP 10 .152.183.61 <none> 8091 /TCP 126m service/lab-ui ClusterIP 10 .152.183.186 <none> 8443 /TCP 126m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/lab-odpi-egeria-lab-presentation 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-nginx 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-ui 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-uistatic 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-jupyter 1 /1 1 1 126m NAME DESIRED CURRENT READY AGE replicaset.apps/lab-odpi-egeria-lab-presentation-bd9789747 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-nginx-7b96949b4f 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-ui-74cc464575 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-uistatic-7b98d4bf9b 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-jupyter-77b6868c4 1 1 1 126m NAME READY AGE statefulset.apps/lab-odpi-egeria-lab-dev 1 /1 126m statefulset.apps/lab-zookeeper 1 /1 126m statefulset.apps/lab-kafka 1 /1 126m statefulset.apps/lab-odpi-egeria-lab-core 1 /1 126m statefulset.apps/lab-odpi-egeria-lab-datalake 1 /1 126m statefulset.apps/lab-odpi-egeria-lab-factory 1 /1 126m ```` All of the ` pod/... ` listed at the top have ` Running ` as their ` STATUS ` and ` 1 /1 ` under ` READY ` . ) ## Accessing the Jupyter notebooks We now need to get connectivity to the interesting pods, such as that running the Jupyer notebook server, as this is where you 'll get to the examples. Since k8s implementations vary one simple approach for local testing is to use `kubectl port-forward` to connect to the relevant service. If you look in the list of services above (`kubectl get services`) we have one named ' service/lab-jupyter ' so let' s try that ( with microk8s ) : ``` shell $ kubectl port-forward service/lab-jupyter 8888 :8888 [ 13 :53:52 ] Forwarding from 127 .0.0.1:8888 -> 8888 Forwarding from [ ::1 ] :8888 -> 8888 This command will not return - the port forwarding is active whilst it's running. In this example we're forwarding all requests to local port 8888 to the k8s service for jupyter At this point you should be able to access your notebooks by going to this forwarded port ie 'http://localhost:8888' Accessing the React UI \u00b6 We repeat the port forwarding above, this time for another service $ kubectl port-forward service/lab-presentation 8091 :8091 [ 14 :15:37 ] Forwarding from 127 .0.0.1:8091 -> 8091 Forwarding from [ ::1 ] :8091 -> 8091 As before, you can define an Ingress, or use nodeports instead if preferred. Now go to https://localhost:8091/coco to access the React UI. Login as 'garygeeke',password 'admin'. Starting over \u00b6 Because the environment is entirely self-contained, you can easily start over the labs simply by deleting the deployment and running the installation again. This will wipe out all of the metadata across the lab Egeria servers, remove all messages from the Kafka bus used in the cohort, reset the Jupyter notebooks to their original clean state, etc. To delete the deployment, simply run this: $ helm delete lab Where lab is the name you used in your original deployment. (You can see what it's called by first running helm list and reviewing the output.) (Then just re-run the last command in the Installation section above to get a fresh environment.) Overriding Configuration \u00b6 The chart is configured to use a default set of parameters. You can override these by creating a file such as lab.yaml with the contents of any values you wish to modify, for example: service: type: NodePort nodeport: jupyter: 30888 core: 30080 datalake: 30081 dev: 30082 factory: 30083 ui: 30443 Refer to the existing values file for additional ports in this section that may reflect new components as added You can then deploy using helm install lab odpi-egeria-lab -f lab.yaml which will override standard defaults with your choices Enabling persistence \u00b6 Support has been added to use persistence in these charts. See 'values.yaml' for more information on this option. You may also wish to refer to the 'egeria-base' helm chart which is a deployment of a single, persistent, autostart server with UI. Note however that since this will save the state of your configuration done from the tutorial notebooks it may be confusing - as such this is disabled by default. It may be useful if you are modifying the charts for your own use. You will also need to delete all storage associated with the chart manually if you want to cleanup/reset - for example kubectl delete pvc --all kubectl delete pv --all License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Lab"},{"location":"charts/lab/#lab-coco-pharmaceuticals-odpi-egeria-lab","text":"This example is intended to replicate the metadata landscape of a hypothetical company, Coco Pharmaceuticals, and allow you to understand a little more about how Egeria can help, and how to use it. This forms our 'Hands on Lab'. The Helm chart will deploy the following components, all in a self-contained environment, allowing you to explore Egeria and its concepts safely and repeatably: Multiple Egeria servers Apache Kafka (and its Zookeeper dependency) Example Jupyter Notebooks Jupyter runtime Egeria's React based UI","title":"Lab - Coco Pharmaceuticals (odpi-egeria-lab)"},{"location":"charts/lab/#prerequisites","text":"In order to use the labs, you'll first need to have the following installed: Kubernetes 1.15 or above Helm 3.0 or above Egeria chart repository configured & updated 6GB RAM minimum is recommended for your k8s environment. You no longer need a git clone of this repository to install the chart.","title":"Prerequisites"},{"location":"charts/lab/#installation","text":"helm install lab egeria/odpi-egeria-lab In the following examples we're using microk8s. $ helm install lab egeria/odpi-egeria-lab [ 11 :47:04 ] WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /var/snap/microk8s/2346/credentials/client.config NAME: lab LAST DEPLOYED: Tue Aug 10 11 :47:19 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ODPi Egeria lab tutorial --- Some additional help text is also output, which is truncated for brevity. It can take a few seconds for the various components to all spin-up. You can monitor the readiness by running kubectl get all -- when ready, you should see output like the following: $ kubectl get all [ 13 :53:43 ] NAME READY STATUS RESTARTS AGE pod/lab-odpi-egeria-lab-ui-74cc464575-cf8rm 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-datalake-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-nginx-7b96949b4f-7dff4 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-presentation-bd9789747-rbv69 1 /1 Running 0 126m pod/lab-kafka-0 1 /1 Running 0 126m pod/lab-zookeeper-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-dev-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-uistatic-7b98d4bf9b-sf9bj 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-core-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-factory-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-jupyter-77b6868c4-9hnlp 1 /1 Running 0 126m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .152.183.1 <none> 443 /TCP 20h service/lab-kafka-headless ClusterIP None <none> 9092 /TCP,9093/TCP 126m service/lab-zookeeper-headless ClusterIP None <none> 2181 /TCP,2888/TCP,3888/TCP 126m service/lab-zookeeper ClusterIP 10 .152.183.25 <none> 2181 /TCP,2888/TCP,3888/TCP 126m service/lab-jupyter ClusterIP 10 .152.183.172 <none> 8888 /TCP 126m service/lab-core ClusterIP 10 .152.183.248 <none> 9443 /TCP 126m service/lab-nginx ClusterIP 10 .152.183.155 <none> 443 /TCP 126m service/lab-datalake ClusterIP 10 .152.183.191 <none> 9443 /TCP 126m service/lab-dev ClusterIP 10 .152.183.122 <none> 9443 /TCP 126m service/lab-kafka ClusterIP 10 .152.183.79 <none> 9092 /TCP 126m service/lab-odpi-egeria-lab-factory ClusterIP 10 .152.183.158 <none> 9443 /TCP 126m service/lab-uistatic ClusterIP 10 .152.183.156 <none> 80 /TCP 126m service/lab-presentation ClusterIP 10 .152.183.61 <none> 8091 /TCP 126m service/lab-ui ClusterIP 10 .152.183.186 <none> 8443 /TCP 126m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/lab-odpi-egeria-lab-presentation 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-nginx 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-ui 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-uistatic 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-jupyter 1 /1 1 1 126m NAME DESIRED CURRENT READY AGE replicaset.apps/lab-odpi-egeria-lab-presentation-bd9789747 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-nginx-7b96949b4f 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-ui-74cc464575 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-uistatic-7b98d4bf9b 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-jupyter-77b6868c4 1 1 1 126m NAME READY AGE statefulset.apps/lab-odpi-egeria-lab-dev 1 /1 126m statefulset.apps/lab-zookeeper 1 /1 126m statefulset.apps/lab-kafka 1 /1 126m statefulset.apps/lab-odpi-egeria-lab-core 1 /1 126m statefulset.apps/lab-odpi-egeria-lab-datalake 1 /1 126m statefulset.apps/lab-odpi-egeria-lab-factory 1 /1 126m ```` All of the ` pod/... ` listed at the top have ` Running ` as their ` STATUS ` and ` 1 /1 ` under ` READY ` . ) ## Accessing the Jupyter notebooks We now need to get connectivity to the interesting pods, such as that running the Jupyer notebook server, as this is where you 'll get to the examples. Since k8s implementations vary one simple approach for local testing is to use `kubectl port-forward` to connect to the relevant service. If you look in the list of services above (`kubectl get services`) we have one named ' service/lab-jupyter ' so let' s try that ( with microk8s ) : ``` shell $ kubectl port-forward service/lab-jupyter 8888 :8888 [ 13 :53:52 ] Forwarding from 127 .0.0.1:8888 -> 8888 Forwarding from [ ::1 ] :8888 -> 8888 This command will not return - the port forwarding is active whilst it's running. In this example we're forwarding all requests to local port 8888 to the k8s service for jupyter At this point you should be able to access your notebooks by going to this forwarded port ie 'http://localhost:8888'","title":"Installation"},{"location":"charts/lab/#accessing-the-react-ui","text":"We repeat the port forwarding above, this time for another service $ kubectl port-forward service/lab-presentation 8091 :8091 [ 14 :15:37 ] Forwarding from 127 .0.0.1:8091 -> 8091 Forwarding from [ ::1 ] :8091 -> 8091 As before, you can define an Ingress, or use nodeports instead if preferred. Now go to https://localhost:8091/coco to access the React UI. Login as 'garygeeke',password 'admin'.","title":"Accessing the React UI"},{"location":"charts/lab/#starting-over","text":"Because the environment is entirely self-contained, you can easily start over the labs simply by deleting the deployment and running the installation again. This will wipe out all of the metadata across the lab Egeria servers, remove all messages from the Kafka bus used in the cohort, reset the Jupyter notebooks to their original clean state, etc. To delete the deployment, simply run this: $ helm delete lab Where lab is the name you used in your original deployment. (You can see what it's called by first running helm list and reviewing the output.) (Then just re-run the last command in the Installation section above to get a fresh environment.)","title":"Starting over"},{"location":"charts/lab/#overriding-configuration","text":"The chart is configured to use a default set of parameters. You can override these by creating a file such as lab.yaml with the contents of any values you wish to modify, for example: service: type: NodePort nodeport: jupyter: 30888 core: 30080 datalake: 30081 dev: 30082 factory: 30083 ui: 30443 Refer to the existing values file for additional ports in this section that may reflect new components as added You can then deploy using helm install lab odpi-egeria-lab -f lab.yaml which will override standard defaults with your choices","title":"Overriding Configuration"},{"location":"charts/lab/#enabling-persistence","text":"Support has been added to use persistence in these charts. See 'values.yaml' for more information on this option. You may also wish to refer to the 'egeria-base' helm chart which is a deployment of a single, persistent, autostart server with UI. Note however that since this will save the state of your configuration done from the tutorial notebooks it may be confusing - as such this is disabled by default. It may be useful if you are modifying the charts for your own use. You will also need to delete all storage associated with the chart manually if you want to cleanup/reset - for example kubectl delete pvc --all kubectl delete pv --all License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Enabling persistence"},{"location":"contribute/devguide/","text":"Contributors \u00b6 If you have ideas for new charts, or have something to contribute, please open up an Issue to discuss. We have also started work on an Egeria Operator at https://github.com/odpi/egeria-k8s-operator & would be delighted if you would like to join there too :-) License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Devguide"},{"location":"contribute/devguide/#contributors","text":"If you have ideas for new charts, or have something to contribute, please open up an Issue to discuss. We have also started work on an Egeria Operator at https://github.com/odpi/egeria-k8s-operator & would be delighted if you would like to join there too :-) License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Contributors"},{"location":"contribute/feedback/","text":"Feedback \u00b6 See Egeria on GitHub for more reference material, our Egeria mailing lists on lists.lfaidata , or our slack channels by joining/signing up at https://slack.lfai.foundation . We'd very much like to help & discuss how we can improve, and ideally how you can help! License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Feedback"},{"location":"contribute/feedback/#feedback","text":"See Egeria on GitHub for more reference material, our Egeria mailing lists on lists.lfaidata , or our slack channels by joining/signing up at https://slack.lfai.foundation . We'd very much like to help & discuss how we can improve, and ideally how you can help! License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Feedback"},{"location":"intro/helm/","text":"Helm \u00b6 Helm is the best way to find, share, and use software built for Kubernetes. - https://helm.sh Deploying apps in Kubernetes \u00b6 In Kubernetes, resources such as pods (to run code) or services (for network accessibility) are defined in yaml. One or more documents can be submitted at a time. So we might have one yaml file that defines our pod - with information about which container images to run, and another to setup a network service. finally we may have another that describes our storage requirements (a persistent volume claim ) What does Helm do? \u00b6 Helm provides a way of bunding yaml files together into an archive, together with a templating mechanism to allow reuse of common patterns & ensure different yamls are consistent and can reference each other. These archives are known as 'Charts' and can be hosted in a known format as a 'chart repository'. The archives are versioned. Helm is basically focussed on creating yaml documents that are submitted to Kubernetes - it is not involved in the runtime of a Kubernetes environment. Once a helm app is installed, interaction is just with the regular kubernetes objects. Helm Commands include options to * install * uninstall * list * search * upgrade * rollback helm under microk8s \u00b6 microk8s qualifies the Helm command helm in that you need to use microk8s helm3 so either * When docs refer you to type helm then just use microk8s helm3 * add a shell alias ie alias helm='microk8s helm3' into ~/.zshrc or equivilent shell startup script - if this doesn't clash with your other usage of k8s. If using this approach you not need to explicitly install Helm. Installing Helm \u00b6 Some Kubernetes environments may install helm as part of their client tooling, refer to the docs to see if this is the case, and run helm version to check - expect to use v3 or above. If so, install can be skipped. MacOS \u00b6 If using macOS with HomeBrew installed, helm can be simply installed with brew install helm Other platforms (Linux, Windows) \u00b6 See the Installation Guide for more ways to install Helm Accessing the egeria charts repository \u00b6 Our helm charts for Egeria are stored in a repository hosted on GitHub. The source for these is at https://github.com/odpi/egeria-charts , and as charts are updated they are automatically published to a GitHub pages Website (in fact this one!) Run the following to add this repository helm repo add egeria https://odpi.github.io/egeria-charts Before searching or installing, always update your local copy of the repository helm repo update egeria You can now list released charts: helm search repo egeria or development charts (being worked on, or using code from master) helm search repo egeria --devel and install a chart that looks interesting - helm install egeria/<chart> License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Helm"},{"location":"intro/helm/#helm","text":"Helm is the best way to find, share, and use software built for Kubernetes. - https://helm.sh","title":"Helm"},{"location":"intro/helm/#deploying-apps-in-kubernetes","text":"In Kubernetes, resources such as pods (to run code) or services (for network accessibility) are defined in yaml. One or more documents can be submitted at a time. So we might have one yaml file that defines our pod - with information about which container images to run, and another to setup a network service. finally we may have another that describes our storage requirements (a persistent volume claim )","title":"Deploying apps in Kubernetes"},{"location":"intro/helm/#what-does-helm-do","text":"Helm provides a way of bunding yaml files together into an archive, together with a templating mechanism to allow reuse of common patterns & ensure different yamls are consistent and can reference each other. These archives are known as 'Charts' and can be hosted in a known format as a 'chart repository'. The archives are versioned. Helm is basically focussed on creating yaml documents that are submitted to Kubernetes - it is not involved in the runtime of a Kubernetes environment. Once a helm app is installed, interaction is just with the regular kubernetes objects. Helm Commands include options to * install * uninstall * list * search * upgrade * rollback","title":"What does Helm do?"},{"location":"intro/helm/#helm-under-microk8s","text":"microk8s qualifies the Helm command helm in that you need to use microk8s helm3 so either * When docs refer you to type helm then just use microk8s helm3 * add a shell alias ie alias helm='microk8s helm3' into ~/.zshrc or equivilent shell startup script - if this doesn't clash with your other usage of k8s. If using this approach you not need to explicitly install Helm.","title":"helm under microk8s"},{"location":"intro/helm/#installing-helm","text":"Some Kubernetes environments may install helm as part of their client tooling, refer to the docs to see if this is the case, and run helm version to check - expect to use v3 or above. If so, install can be skipped.","title":"Installing Helm"},{"location":"intro/helm/#macos","text":"If using macOS with HomeBrew installed, helm can be simply installed with brew install helm","title":"MacOS"},{"location":"intro/helm/#other-platforms-linux-windows","text":"See the Installation Guide for more ways to install Helm","title":"Other platforms (Linux, Windows)"},{"location":"intro/helm/#accessing-the-egeria-charts-repository","text":"Our helm charts for Egeria are stored in a repository hosted on GitHub. The source for these is at https://github.com/odpi/egeria-charts , and as charts are updated they are automatically published to a GitHub pages Website (in fact this one!) Run the following to add this repository helm repo add egeria https://odpi.github.io/egeria-charts Before searching or installing, always update your local copy of the repository helm repo update egeria You can now list released charts: helm search repo egeria or development charts (being worked on, or using code from master) helm search repo egeria --devel and install a chart that looks interesting - helm install egeria/<chart> License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing the egeria charts repository"},{"location":"intro/k8s/","text":"What is Kubernetes? \u00b6 Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. - https://kubernetes.io This is how the official website describes it. It's effectively a standardized way of deploying applications in a very scalable way - from everything such as development prototyping through to massive highly available enterprise solutions. In this document I'll give a very brief summary that should help those of you new to Kubernetes to make your first steps with Egeria. What are the key concepts in Kubernetes? \u00b6 These are just some of the concepts that can help to understand what's going on. This isn't a complete list. Api \u00b6 Kubernetes using a standard API which is oriented around manipulating Objects. The commands are therefore very standard, it's all about the objects. Making it so \u00b6 The system is always observing the state of the system through these objects, and where there are discrepancies, taking action to 'Make it So' as Captain Picard would say. The approach is imperitive. So we think of these objects as describing the desired state of the system. Namespace \u00b6 A namespace provides a way of seperating out kubernetes resources by users or applications as a convenience. It keeps names more understandable, and avoids global duplicates. For example a developer working on a k8s cluster may have a namespace of their own to experiment in. Container \u00b6 A Container is what runs stuff. It's similar to a Virtual Machine in some ways, but much more lightweight. Containers use code Images which may be custom built, or very standard off-the-shelf reusable items. Containers are typically very focussed on a single need or application. Pod \u00b6 A Pod is a single group of one or more containers. Typically a single main container runs in a pod, but this may be supported by additional containers for log, audit, security, initialization etc. Think of this as an atomic unit that can run a workload. Pods are disposeable - they will come and go. Other objects are concerned with providing a reliable service. Service \u00b6 A service provides network accessibility to one or more pods. The service name will be added into local Domain Name Service (DNS) for easy accessibility from other pods. Load can be shared across multiple pods Ingres \u00b6 Think of Ingress as the entry point to Kubernetes services from an external network perspective - so it is these addresses external users would be aware of. Deployment \u00b6 A deployment keeps a set of pods running - including replica copies, ie restarted if stopped, matching resource requirements, handling node failure . Stateful Set \u00b6 A stateful set goes further than a deployment in that it keeps a well known identifier for each identical replica. This helps in allocating persistent storage & network resources to a replica ConfigMap \u00b6 A config map is a way of keeping configuration (exposed as files or environment variables) seperate to an application. Secret \u00b6 A secret is used to keep information secret, as the name might suggest ... This might be a password or an API key & the data is encrypted Custom Objects \u00b6 In addition to this list -- and many more covered in the official documentation -- Kubernetes also supports custom resources. These form a key part of Kubernetes Operators . Storage \u00b6 Pods can request storage - which is known as a persistent volume claim (PVC), which are either manually or automatically resolved to a persistent volume. See the k8s docs Persistent Volumes Why are we using Kubernetes? \u00b6 All sizes of systems can run kubernetes applications - from a small raspberry pi through desktops and workstations through to huge cloud deployments. Whilst the details around storage, security, networking etc do vary by implementation, the core concepts, and configurations work across all. Some may be more concerned about an easy way to play with development code, try out new ideas, whilst at the far end of the spectrum enterprises want something super scalable and reliable, and easy to monitor. For egeria we want to achieve two main things * Provide easy to use demos and tutorials that show how Egeria can be used and worked with without requiring too much complex setup. * Provide examples that show how Egeria can be deployed in k8s, and then adapted for the organization's needs. Other alternatives that might come to mind include * Docker -- whilst simple, this is more geared around running a single container, and building complex environment means a lot of work combining application stacks together, often resulting in something that isn't usable. We do of course still have container images, which are essential to k8s, but these are simple & self contained. * docker-compose -- this builds on docker in allowing multiple containers and servers to be orchestrated, but it's much less flexible & scalable than kubernetes. How do I get access to Kubernetes? \u00b6 Getting Started provides links to setting up Kubernetes in many environments. Below we'll take a quick look at some of the simpler examples, especially for new users. microk8s (Linux, Windows, macOS) \u00b6 Official microk8s site 4GB is recommended as a minimum memory requirement. As with most k8s implementations, when running some ongoing cpu will be used, so if running on your laptop/low power device it's recommended to refer to the relevant docs & stop k8s when not in use. When running on a separate server or a cloud service this isn't a concern. When using microk8s, note that the standard k8s commands are renamed to avoid clashes, so use the microk8s ones in the remainder of the Egeria documentation kubectl -> microk8s kubectl helm -> microk8s helm They can also be aliased on some platforms. MacOS \u00b6 The macos install docs cover the steps needed to install microk8s. Most of the Egeria development team use MacOS, so the instructions are elaborated and qualified here: The recommended approach uses HomeBrew . This offers a suite of tools often found on linux which are easy to setup on macOS. See install docs IMPORTANT: Before installing, go into System Preferences->Security & Privacy. Click the lock to get into Admin mode. Then ensure Firewall Options->Enable Stealth Mode is NOT enabled (no tick). If it is, microk8s will not work properly . More If you are concerned over the firewall change, or homebrew requirement, refer back to the official k8s documentation & choose another k8s implementation that works for you. Ensure you turn on the following services: storage, dns, helm3 . dashboard is also useful to understand more about k8s and what is running. However it is currently failing as described in issue 2507 As an example, the following commands should get you set up, but always check the official docs for current details brew install ubuntu/microk8s/microk8s microk8s install microk8s status --wait-ready microk8s enable dns storage helm3 microk8s kubectl get all --all-namespaces Kubernetes is now running. Windows \u00b6 Follow the official instructions (untested) Linux \u00b6 Follow the official instructions (untested) kubectl command under microk8s \u00b6 microk8s qualifies the core k8s command kubectl in that you need to use microk8s kubectl so either * When docs refer you to type kubectl then just use microk8s kubectl * add a shell alias ie alias kubectl='microk8s kubectl' into ~/.zshrc or equivilent shell startup script Docker Desktop (Windows, macOS) \u00b6 Docker Desktop supports Kubernetes After installing, go into Docker Desktop 'settings and select 'Kubernetes'. Make sure 'Enable Kubernetes' is checked. Also under resources ensure at least 4GB is allocated to Docker Cloud \u00b6 Many cloud providers offer Kubernetes deployments which can be used for experimentation or production. This include Redhat OpenShift on multiple cloud providers including on IBMCloud Kubernetes on IBMCloud Azure Kubernetes Service Google Kubernetes Engine (GKE) In addition to a cloud install, ensure you have installed the relevant cloud provider's tooling to manage their k8s environment, including having access to the standard kubernetes command kubectl . Note that in the team's testing we mostly are running Redhat OpenShift on IBMCloud as a managed service. We welcome feedback of running our examples on other environments, especially as some of the specifics around ingress rules, storage, security can vary. Accessing applications in your cluster \u00b6 See also kubernetes docs NodePort \u00b6 In the sample charts provided an option to use a NodePort is usually provided. This is often easiest when running k8s locally, as any of the ip addressible worker nodes in your cluster can service a request on the port provided. This is why it's named a 'node port' ie a port on your node.. kubectl port-forward \u00b6 This can be run at a command line, and directly sets up forwarding from local ports into services running in your cluster. It requires no additional configuration beforehand, and lasts only as long as the port forwarding is running. See port forwarding for more info Ingress \u00b6 Ingress rules define how traffic directed at your k8s cluster is directed. Their definition tends to vary substantially between different k8s implementations but often is the easiest approach when running with a cloud service. * microk8s ingress \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"K8s"},{"location":"intro/k8s/#what-is-kubernetes","text":"Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. - https://kubernetes.io This is how the official website describes it. It's effectively a standardized way of deploying applications in a very scalable way - from everything such as development prototyping through to massive highly available enterprise solutions. In this document I'll give a very brief summary that should help those of you new to Kubernetes to make your first steps with Egeria.","title":"What is Kubernetes?"},{"location":"intro/k8s/#what-are-the-key-concepts-in-kubernetes","text":"These are just some of the concepts that can help to understand what's going on. This isn't a complete list.","title":"What are the key concepts in Kubernetes?"},{"location":"intro/k8s/#api","text":"Kubernetes using a standard API which is oriented around manipulating Objects. The commands are therefore very standard, it's all about the objects.","title":"Api"},{"location":"intro/k8s/#making-it-so","text":"The system is always observing the state of the system through these objects, and where there are discrepancies, taking action to 'Make it So' as Captain Picard would say. The approach is imperitive. So we think of these objects as describing the desired state of the system.","title":"Making it so"},{"location":"intro/k8s/#namespace","text":"A namespace provides a way of seperating out kubernetes resources by users or applications as a convenience. It keeps names more understandable, and avoids global duplicates. For example a developer working on a k8s cluster may have a namespace of their own to experiment in.","title":"Namespace"},{"location":"intro/k8s/#container","text":"A Container is what runs stuff. It's similar to a Virtual Machine in some ways, but much more lightweight. Containers use code Images which may be custom built, or very standard off-the-shelf reusable items. Containers are typically very focussed on a single need or application.","title":"Container"},{"location":"intro/k8s/#pod","text":"A Pod is a single group of one or more containers. Typically a single main container runs in a pod, but this may be supported by additional containers for log, audit, security, initialization etc. Think of this as an atomic unit that can run a workload. Pods are disposeable - they will come and go. Other objects are concerned with providing a reliable service.","title":"Pod"},{"location":"intro/k8s/#service","text":"A service provides network accessibility to one or more pods. The service name will be added into local Domain Name Service (DNS) for easy accessibility from other pods. Load can be shared across multiple pods","title":"Service"},{"location":"intro/k8s/#ingres","text":"Think of Ingress as the entry point to Kubernetes services from an external network perspective - so it is these addresses external users would be aware of.","title":"Ingres"},{"location":"intro/k8s/#deployment","text":"A deployment keeps a set of pods running - including replica copies, ie restarted if stopped, matching resource requirements, handling node failure .","title":"Deployment"},{"location":"intro/k8s/#stateful-set","text":"A stateful set goes further than a deployment in that it keeps a well known identifier for each identical replica. This helps in allocating persistent storage & network resources to a replica","title":"Stateful Set"},{"location":"intro/k8s/#configmap","text":"A config map is a way of keeping configuration (exposed as files or environment variables) seperate to an application.","title":"ConfigMap"},{"location":"intro/k8s/#secret","text":"A secret is used to keep information secret, as the name might suggest ... This might be a password or an API key & the data is encrypted","title":"Secret"},{"location":"intro/k8s/#custom-objects","text":"In addition to this list -- and many more covered in the official documentation -- Kubernetes also supports custom resources. These form a key part of Kubernetes Operators .","title":"Custom Objects"},{"location":"intro/k8s/#storage","text":"Pods can request storage - which is known as a persistent volume claim (PVC), which are either manually or automatically resolved to a persistent volume. See the k8s docs Persistent Volumes","title":"Storage"},{"location":"intro/k8s/#why-are-we-using-kubernetes","text":"All sizes of systems can run kubernetes applications - from a small raspberry pi through desktops and workstations through to huge cloud deployments. Whilst the details around storage, security, networking etc do vary by implementation, the core concepts, and configurations work across all. Some may be more concerned about an easy way to play with development code, try out new ideas, whilst at the far end of the spectrum enterprises want something super scalable and reliable, and easy to monitor. For egeria we want to achieve two main things * Provide easy to use demos and tutorials that show how Egeria can be used and worked with without requiring too much complex setup. * Provide examples that show how Egeria can be deployed in k8s, and then adapted for the organization's needs. Other alternatives that might come to mind include * Docker -- whilst simple, this is more geared around running a single container, and building complex environment means a lot of work combining application stacks together, often resulting in something that isn't usable. We do of course still have container images, which are essential to k8s, but these are simple & self contained. * docker-compose -- this builds on docker in allowing multiple containers and servers to be orchestrated, but it's much less flexible & scalable than kubernetes.","title":"Why are we using Kubernetes?"},{"location":"intro/k8s/#how-do-i-get-access-to-kubernetes","text":"Getting Started provides links to setting up Kubernetes in many environments. Below we'll take a quick look at some of the simpler examples, especially for new users.","title":"How do I get access to Kubernetes?"},{"location":"intro/k8s/#microk8s-linux-windows-macos","text":"Official microk8s site 4GB is recommended as a minimum memory requirement. As with most k8s implementations, when running some ongoing cpu will be used, so if running on your laptop/low power device it's recommended to refer to the relevant docs & stop k8s when not in use. When running on a separate server or a cloud service this isn't a concern. When using microk8s, note that the standard k8s commands are renamed to avoid clashes, so use the microk8s ones in the remainder of the Egeria documentation kubectl -> microk8s kubectl helm -> microk8s helm They can also be aliased on some platforms.","title":"microk8s (Linux, Windows, macOS)"},{"location":"intro/k8s/#macos","text":"The macos install docs cover the steps needed to install microk8s. Most of the Egeria development team use MacOS, so the instructions are elaborated and qualified here: The recommended approach uses HomeBrew . This offers a suite of tools often found on linux which are easy to setup on macOS. See install docs IMPORTANT: Before installing, go into System Preferences->Security & Privacy. Click the lock to get into Admin mode. Then ensure Firewall Options->Enable Stealth Mode is NOT enabled (no tick). If it is, microk8s will not work properly . More If you are concerned over the firewall change, or homebrew requirement, refer back to the official k8s documentation & choose another k8s implementation that works for you. Ensure you turn on the following services: storage, dns, helm3 . dashboard is also useful to understand more about k8s and what is running. However it is currently failing as described in issue 2507 As an example, the following commands should get you set up, but always check the official docs for current details brew install ubuntu/microk8s/microk8s microk8s install microk8s status --wait-ready microk8s enable dns storage helm3 microk8s kubectl get all --all-namespaces Kubernetes is now running.","title":"MacOS"},{"location":"intro/k8s/#windows","text":"Follow the official instructions (untested)","title":"Windows"},{"location":"intro/k8s/#linux","text":"Follow the official instructions (untested)","title":"Linux"},{"location":"intro/k8s/#kubectl-command-under-microk8s","text":"microk8s qualifies the core k8s command kubectl in that you need to use microk8s kubectl so either * When docs refer you to type kubectl then just use microk8s kubectl * add a shell alias ie alias kubectl='microk8s kubectl' into ~/.zshrc or equivilent shell startup script","title":"kubectl command under microk8s"},{"location":"intro/k8s/#docker-desktop-windows-macos","text":"Docker Desktop supports Kubernetes After installing, go into Docker Desktop 'settings and select 'Kubernetes'. Make sure 'Enable Kubernetes' is checked. Also under resources ensure at least 4GB is allocated to Docker","title":"Docker Desktop (Windows, macOS)"},{"location":"intro/k8s/#cloud","text":"Many cloud providers offer Kubernetes deployments which can be used for experimentation or production. This include Redhat OpenShift on multiple cloud providers including on IBMCloud Kubernetes on IBMCloud Azure Kubernetes Service Google Kubernetes Engine (GKE) In addition to a cloud install, ensure you have installed the relevant cloud provider's tooling to manage their k8s environment, including having access to the standard kubernetes command kubectl . Note that in the team's testing we mostly are running Redhat OpenShift on IBMCloud as a managed service. We welcome feedback of running our examples on other environments, especially as some of the specifics around ingress rules, storage, security can vary.","title":"Cloud"},{"location":"intro/k8s/#accessing-applications-in-your-cluster","text":"See also kubernetes docs","title":"Accessing applications in your cluster"},{"location":"intro/k8s/#nodeport","text":"In the sample charts provided an option to use a NodePort is usually provided. This is often easiest when running k8s locally, as any of the ip addressible worker nodes in your cluster can service a request on the port provided. This is why it's named a 'node port' ie a port on your node..","title":"NodePort"},{"location":"intro/k8s/#kubectl-port-forward","text":"This can be run at a command line, and directly sets up forwarding from local ports into services running in your cluster. It requires no additional configuration beforehand, and lasts only as long as the port forwarding is running. See port forwarding for more info","title":"kubectl port-forward"},{"location":"intro/k8s/#ingress","text":"Ingress rules define how traffic directed at your k8s cluster is directed. Their definition tends to vary substantially between different k8s implementations but often is the easiest approach when running with a cloud service.","title":"Ingress"},{"location":"intro/k8s/#microk8s-ingress","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"* microk8s ingress"},{"location":"more/operator/","text":"We have also started work on an Egeria Operator at https://github.com/odpi/egeria-k8s-operator & would be delighted if you would like to join there too :-) License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Operator"}]}