{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"template: home.html title: About License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"About"},{"location":"charts/base/","text":"Base Egeria (egeria-base) \u00b6 This is a simple deployment of Egeria which will just deploy a basic Egeria environment which is ready for you to experiment with. Specifically it sets up A single egeria platform which hosts An egeria metadata server with a persistent graph store A new server to support the UI A UI to allow browsing of types, instances & servers Apache Kafka & Zookeeper It does not provide access to our lab notebooks, the Polymer based UI, nor is it preloaded with any data. This chart may also be useful to understand how to deploy Egeria within kubernetes. In future we anticipate providing an operator which will be more flexible Prerequisites \u00b6 In order to use the labs, you'll first need to have the following installed: A Kubernetes cluster at 1.15 or above the kubectl tool in your path Helm 3.0 or above No configuration of the chart is required to use defaults, but information is provided below Installation \u00b6 From one directory level above the location of this README, run the following: helm dep update egeria-base helm install egeria egeria-base THE INSTALL WILL TAKE SEVERAL MINUTES This is because it is not only creating the required objects in Kubernetes to run the platforms, but also is configuring egeria itself - which involves waiting for everything to startup before configuring Egeria via REST API calls. Once installed the configured server is set to start automatically, storage is persisted, and so if your pod gets moved/restarted, egeria should come back automatically with the same data as before. Additional Install Configuration \u00b6 This section is optional - skip over if you're happy with defaults - a good idea to begin with. In a helm chart the configuration that has been externalised by the chart writer is specified in the values.yaml file which you can find in this directory. However rather than edit this file directly, it's recommended you create an additional file with the required overrides. As an example, in values.yaml we see a value 'serverName' which is set to mds1. If I want to override this I could do helm install --set-string egeria.serverName = myserver However this can get tedious with multiple values to override, and you need to know the correct types to use. Instead it may be easier to create an additional file. For example let's create a file in my home directory ~/egeria.yaml containing: egeria: serverName: metadataserver viewServerName: presentationview We can then install the chart with: helm install -f ~/egeria.yaml egeria egeria-base Up to now the installation has been called egeria but we could call it something else ie metadataserver helm install -f ~/egeria.yaml metadataserver egeria-base This is known as a release in Helm, and we can have multiple installed currently. To list these use helm list and to delete both names we've experimented with so far: helm delete metadataserver helm delete egeria Refer to the comments in values.yaml for further information on what can be configured - this includes: - server, organization, cohort names - storage options - k8s storage class/size - Egeria version - Kubernetes service setup (see below also) - roles & accounts - timeouts - names & repositories for docker images used by the chart - Kafka specific configuration (setup in the Bitnami chart we use) Using additional connectors \u00b6 If you have additional Egeria connectors that are needed in your deployment, you should soon be able to include the following when deploying the chart helm install --include-dir libs = ~/libs egeria egeria-base Where in this example ~/libs is the directory including the additional connectors you wish to use. However the support for this is awaiting a helm PR , so in the meantime please copy files directly into a 'libs' directory within the chart instead. For example mkdir egeria-base/libs cp ~/libs/*jar egeria-base/libs These files will be copied into a kubernetes config map, which is then made available as a mounted volume to the runtime image of egeria & added to the class loading path as /extlib . You still need to configure the egeria server(s) appropriately Accessing Egeria \u00b6 When this chart is installed, an initialization job is run to configure the egeria metadata server and UI. For example looking at kubernetes jobs will show something like: $ kubectl get pods [ 17 :27:11 ] NAME READY STATUS RESTARTS AGE egeria-base-config-crhrv 1 /1 Running 0 4m16s egeria-base-platform-0 1 /1 Running 0 4m16s egeria-base-presentation-699669cfd4-9swjb 1 /1 Running 0 4m16s egeria-kafka-0 1 /1 Running 2 4m16s egeria-zookeeper-0 1 /1 Running 0 4m16s You should wait until that first 'egeria-base-config' job completes, it will then disappear from the list. This job issues REST calls against the egeria serves to configure them for this simple environment (see scripts directory). The script will not run again, since we will have now configured the servers with persistent storage, and for the platform to autostart our servers. So even if a pod is removed and restarted, the egeria platform and servers should return in the same state. We now have egeria running within a Kubernetes cluster, but by default no services are exposed externally - they are all of type ClusterIP - we can see these with $ kubectl get services [ 12 :38:16 ] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE egeria-kafka ClusterIP 172 .21.42.105 <none> 9092 /TCP 33m egeria-kafka-headless ClusterIP None <none> 9092 /TCP,9093/TCP 33m egeria-platform ClusterIP 172 .21.40.79 <none> 9443 /TCP 33m egeria-presentation ClusterIP 172 .21.107.242 <none> 8091 /TCP 33m egeria-zookeeper ClusterIP 172 .21.126.56 <none> 2181 /TCP,2888/TCP,3888/TCP 33m egeria-zookeeper-headless ClusterIP None <none> 2181 /TCP,2888/TCP,3888/TCP 33m The egeria-presentation service is very useful to expose as this provides a useful UI where you can explore types and instances. Also egeria-platform is the service for egeria itself. In production you might want this only exposed very carefully to other systems - and not other users or the internet, but for experimenting with egeria let's assume you do. How these are exposed can be somewhat dependent on the specific kubernetes environment you are using. As an example, when running RedHat OpenShift in IBM Cloud, you can expose these services via a LoadBalancer using kubectl expose service/egeria-presentation --type=LoadBalancer --port=8091 --target-port=8091 --name pres kubectl expose service/egeria-platform --type=LoadBalancer --port=9443 --target-port=9443 --name platform If I run these, and then look again at my services I see I now have 2 additional entries (modified for obfuscation): platform LoadBalancer 172.21.218.241 3bc644c3-eu-gb.lb.appdomain.cloud 9443:30640/TCP 25h pres LoadBalancer 172.21.18.10 42b0c7f5-eu-gb.lb.appdomain.cloud 8091:30311/TCP 22h So I can access them at their respective hosts. Note that where hosts are allocated dynamically, it can take up to an hour or more for DNS caches to refresh. Waiting up to 5 minutes then refreshing a local cache has proven sufficient for me, but your experience may differ. In the example above, the Egeria UI can be accessed at https://42b0c7f5-eu-gb.lb.appdomain.cloud:8091/org/ . Replace the hostname accordingly, and also the org is the organization name from the Values.yaml file we referred to above Once logged in, you should be able to login using our demo userid/password of garygeeke/admin & start browsing types and instances within Egeria. You can also issue REST API calls against egeria using a base URL for the platform of https://3bc644c3-eu-gb.lb.appdomain.cloud - Other material covers these REST API calls in more detail, but a simple api doc is available at https://3bc644c3-eu-gb.lb.appdomain.cloud/swagger-ui.html Cleaning up / removing Egeria \u00b6 To delete the deployment, simply run this for Helm3: $ helm delete egeria In addition, the egeria chart makes use of persistent storage. To remove these use $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-egeria-kafka-0 Bound pvc-d31e0012-8568-4e6f-ae5d-94832ebcd92d 10Gi RWO ibmc-vpc-block-10iops-tier 48m data-egeria-zookeeper-0 Bound pvc-90739fce-dce0-422b-8738-a38293d8fdfb 10Gi RWO ibmc-vpc-block-10iops-tier 48m egeria-egeria-data-egeria-base-platform-0 Bound pvc-197ba47e-a6d5-4f35-a7d8-7b7ec1ed1df3 10Gi RWO ibmc-vpc-block-10iops-tier 48m Identify those associated with egeria - which should be obvious from the name and then delete with kubectl delete pvc <id> See the section on Configuration for more details Feedback & Future \u00b6 See Egeria on GitHub for more reference material, our Egeria mailing lists on lists.lfaidata , or our slack channels by joining/singing up at https://slack.lfai.foundation . We'd very much like to help & discuss how we can improve, and ideally how you can help! This helm chart offers a basic configuration only - we have also started work at https://github.com/odpi/egeria-k8s-operator & would be delighted if you would like to join there too :-) License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Base"},{"location":"charts/base/#base-egeria-egeria-base","text":"This is a simple deployment of Egeria which will just deploy a basic Egeria environment which is ready for you to experiment with. Specifically it sets up A single egeria platform which hosts An egeria metadata server with a persistent graph store A new server to support the UI A UI to allow browsing of types, instances & servers Apache Kafka & Zookeeper It does not provide access to our lab notebooks, the Polymer based UI, nor is it preloaded with any data. This chart may also be useful to understand how to deploy Egeria within kubernetes. In future we anticipate providing an operator which will be more flexible","title":"Base Egeria (egeria-base)"},{"location":"charts/base/#prerequisites","text":"In order to use the labs, you'll first need to have the following installed: A Kubernetes cluster at 1.15 or above the kubectl tool in your path Helm 3.0 or above No configuration of the chart is required to use defaults, but information is provided below","title":"Prerequisites"},{"location":"charts/base/#installation","text":"From one directory level above the location of this README, run the following: helm dep update egeria-base helm install egeria egeria-base THE INSTALL WILL TAKE SEVERAL MINUTES This is because it is not only creating the required objects in Kubernetes to run the platforms, but also is configuring egeria itself - which involves waiting for everything to startup before configuring Egeria via REST API calls. Once installed the configured server is set to start automatically, storage is persisted, and so if your pod gets moved/restarted, egeria should come back automatically with the same data as before.","title":"Installation"},{"location":"charts/base/#additional-install-configuration","text":"This section is optional - skip over if you're happy with defaults - a good idea to begin with. In a helm chart the configuration that has been externalised by the chart writer is specified in the values.yaml file which you can find in this directory. However rather than edit this file directly, it's recommended you create an additional file with the required overrides. As an example, in values.yaml we see a value 'serverName' which is set to mds1. If I want to override this I could do helm install --set-string egeria.serverName = myserver However this can get tedious with multiple values to override, and you need to know the correct types to use. Instead it may be easier to create an additional file. For example let's create a file in my home directory ~/egeria.yaml containing: egeria: serverName: metadataserver viewServerName: presentationview We can then install the chart with: helm install -f ~/egeria.yaml egeria egeria-base Up to now the installation has been called egeria but we could call it something else ie metadataserver helm install -f ~/egeria.yaml metadataserver egeria-base This is known as a release in Helm, and we can have multiple installed currently. To list these use helm list and to delete both names we've experimented with so far: helm delete metadataserver helm delete egeria Refer to the comments in values.yaml for further information on what can be configured - this includes: - server, organization, cohort names - storage options - k8s storage class/size - Egeria version - Kubernetes service setup (see below also) - roles & accounts - timeouts - names & repositories for docker images used by the chart - Kafka specific configuration (setup in the Bitnami chart we use)","title":"Additional Install Configuration"},{"location":"charts/base/#using-additional-connectors","text":"If you have additional Egeria connectors that are needed in your deployment, you should soon be able to include the following when deploying the chart helm install --include-dir libs = ~/libs egeria egeria-base Where in this example ~/libs is the directory including the additional connectors you wish to use. However the support for this is awaiting a helm PR , so in the meantime please copy files directly into a 'libs' directory within the chart instead. For example mkdir egeria-base/libs cp ~/libs/*jar egeria-base/libs These files will be copied into a kubernetes config map, which is then made available as a mounted volume to the runtime image of egeria & added to the class loading path as /extlib . You still need to configure the egeria server(s) appropriately","title":"Using additional connectors"},{"location":"charts/base/#accessing-egeria","text":"When this chart is installed, an initialization job is run to configure the egeria metadata server and UI. For example looking at kubernetes jobs will show something like: $ kubectl get pods [ 17 :27:11 ] NAME READY STATUS RESTARTS AGE egeria-base-config-crhrv 1 /1 Running 0 4m16s egeria-base-platform-0 1 /1 Running 0 4m16s egeria-base-presentation-699669cfd4-9swjb 1 /1 Running 0 4m16s egeria-kafka-0 1 /1 Running 2 4m16s egeria-zookeeper-0 1 /1 Running 0 4m16s You should wait until that first 'egeria-base-config' job completes, it will then disappear from the list. This job issues REST calls against the egeria serves to configure them for this simple environment (see scripts directory). The script will not run again, since we will have now configured the servers with persistent storage, and for the platform to autostart our servers. So even if a pod is removed and restarted, the egeria platform and servers should return in the same state. We now have egeria running within a Kubernetes cluster, but by default no services are exposed externally - they are all of type ClusterIP - we can see these with $ kubectl get services [ 12 :38:16 ] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE egeria-kafka ClusterIP 172 .21.42.105 <none> 9092 /TCP 33m egeria-kafka-headless ClusterIP None <none> 9092 /TCP,9093/TCP 33m egeria-platform ClusterIP 172 .21.40.79 <none> 9443 /TCP 33m egeria-presentation ClusterIP 172 .21.107.242 <none> 8091 /TCP 33m egeria-zookeeper ClusterIP 172 .21.126.56 <none> 2181 /TCP,2888/TCP,3888/TCP 33m egeria-zookeeper-headless ClusterIP None <none> 2181 /TCP,2888/TCP,3888/TCP 33m The egeria-presentation service is very useful to expose as this provides a useful UI where you can explore types and instances. Also egeria-platform is the service for egeria itself. In production you might want this only exposed very carefully to other systems - and not other users or the internet, but for experimenting with egeria let's assume you do. How these are exposed can be somewhat dependent on the specific kubernetes environment you are using. As an example, when running RedHat OpenShift in IBM Cloud, you can expose these services via a LoadBalancer using kubectl expose service/egeria-presentation --type=LoadBalancer --port=8091 --target-port=8091 --name pres kubectl expose service/egeria-platform --type=LoadBalancer --port=9443 --target-port=9443 --name platform If I run these, and then look again at my services I see I now have 2 additional entries (modified for obfuscation): platform LoadBalancer 172.21.218.241 3bc644c3-eu-gb.lb.appdomain.cloud 9443:30640/TCP 25h pres LoadBalancer 172.21.18.10 42b0c7f5-eu-gb.lb.appdomain.cloud 8091:30311/TCP 22h So I can access them at their respective hosts. Note that where hosts are allocated dynamically, it can take up to an hour or more for DNS caches to refresh. Waiting up to 5 minutes then refreshing a local cache has proven sufficient for me, but your experience may differ. In the example above, the Egeria UI can be accessed at https://42b0c7f5-eu-gb.lb.appdomain.cloud:8091/org/ . Replace the hostname accordingly, and also the org is the organization name from the Values.yaml file we referred to above Once logged in, you should be able to login using our demo userid/password of garygeeke/admin & start browsing types and instances within Egeria. You can also issue REST API calls against egeria using a base URL for the platform of https://3bc644c3-eu-gb.lb.appdomain.cloud - Other material covers these REST API calls in more detail, but a simple api doc is available at https://3bc644c3-eu-gb.lb.appdomain.cloud/swagger-ui.html","title":"Accessing Egeria"},{"location":"charts/base/#cleaning-up-removing-egeria","text":"To delete the deployment, simply run this for Helm3: $ helm delete egeria In addition, the egeria chart makes use of persistent storage. To remove these use $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-egeria-kafka-0 Bound pvc-d31e0012-8568-4e6f-ae5d-94832ebcd92d 10Gi RWO ibmc-vpc-block-10iops-tier 48m data-egeria-zookeeper-0 Bound pvc-90739fce-dce0-422b-8738-a38293d8fdfb 10Gi RWO ibmc-vpc-block-10iops-tier 48m egeria-egeria-data-egeria-base-platform-0 Bound pvc-197ba47e-a6d5-4f35-a7d8-7b7ec1ed1df3 10Gi RWO ibmc-vpc-block-10iops-tier 48m Identify those associated with egeria - which should be obvious from the name and then delete with kubectl delete pvc <id> See the section on Configuration for more details","title":"Cleaning up / removing Egeria"},{"location":"charts/base/#feedback-future","text":"See Egeria on GitHub for more reference material, our Egeria mailing lists on lists.lfaidata , or our slack channels by joining/singing up at https://slack.lfai.foundation . We'd very much like to help & discuss how we can improve, and ideally how you can help! This helm chart offers a basic configuration only - we have also started work at https://github.com/odpi/egeria-k8s-operator & would be delighted if you would like to join there too :-) License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Feedback &amp; Future"},{"location":"charts/lab/","text":"Lab - Coco Pharmaceuticals (odpi-egeria-lab) \u00b6 This Helm chart will deploy the following components, all in a self-contained environment, for use in the Egeria hands-on labs -- allowing you to explore Egeria and its concepts safely and repeatably: Multiple Egeria servers Apache Kafka (and its Zookeeper dependency) Jupyter Notebooks Prerequisites \u00b6 In order to use the labs, you'll first need to have the following installed: Kubernetes 1.15 or above Helm 3.0 or above The minimum tested configurations for Kubernetes are - Cloud/remote service - 3 nodes, 2GB ram per node - Local docker for mac/windows - 1 node, 6Gb ram dedicated to docker You could use the Docker-embedded Kubernetes for this on eg. Docker Desktop, or a public cloud service that provides Kubernetes If you need to install helm3, please obtain from https://github.com/helm/helm/releases before starting and ensure the 'helm' executable is in your PATH. The instructions and examples that follow assume use of this version If you can't meet these requirements, or would like to start with a simpler approach, an alternative environment for running the tutorials has been implemented using docker-compose & can be found at https://github.com/odpi/egeria/tree/master/open-metadata-resources/open-metadata-deployment/compose/tutorials . Installation \u00b6 From one directory level above the location of this README, run the following: helm dep update odpi-egeria-lab helm install lab odpi-egeria-lab Example transcript: $ export PATH=~/bin:$PATH $ pwd /home/jonesn/egeria/open-metadata-resources/open-metadata-deployment/charts $ helm version version.BuildInfo{Version:\"v3.5.3\", GitCommit:\"041ce5a2c17a58be0fcd5f5e16fb3e7e95fea622\", GitTreeState:\"dirty\", GoVersion:\"go1.16\"} $ helm dep update odpi-egeria-lab Getting updates for unmanaged Helm repositories... ...Successfully got an update from the \"https://charts.bitnami.com/bitnami\" chart repository Saving 1 charts Downloading kafka from repo https://charts.bitnami.com/bitnami Deleting outdated charts Now we can actually do the deployment with helm install lab odpi-egeria-lab which will look like: $ helm install lab odpi-egeria-lab [16:52:10] NAME: lab LAST DEPLOYED: Tue Apr 6 16:52:17 2021 NAMESPACE: egeria-release STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ODPi Egeria lab tutorial --- The Egeria tutorials have now been deployed to Kubernetes. It may take a minute or so for everything to start up. Open your web browser and go to addressofmycluster:30888 to get started You may need to contact your cluster admin, or read your cloud provider helptext to understand the correct 'addressofmycluster' to use. If you experience problems, check memory consumption on your nodes. A minimum of a 3 node cluster, 2GB per node; or a desktop environment with 6GB dedicated is recommended. Please provide any feeback via a github issue at https://github.com/odpi/egeria or join us on slack via https://http://slack.lfai.foundation - The ODPi Egeria team Note that it can take a few seconds for the various components to all spin-up. You can monitor the readiness by running kubectl get all -- when ready, you should see output like the following: $ kubectl get all [16:52:24] NAME READY STATUS RESTARTS AGE pod/lab-odpi-egeria-lab-core-0 0/1 Running 0 34s pod/lab-odpi-egeria-lab-datalake-0 0/1 Running 0 34s pod/lab-odpi-egeria-lab-dev-0 0/1 Running 0 34s pod/lab-odpi-egeria-lab-factory-0 0/1 Running 0 34s pod/lab-odpi-egeria-lab-jupyter-bb7cf5f47-k6w2p 1/1 Running 0 34s pod/lab-odpi-egeria-lab-nginx-69f7cfc67b-nptz5 1/1 Running 0 34s pod/lab-odpi-egeria-lab-presentation-66574b9796-7t4rc 1/1 Running 0 34s pod/lab-odpi-egeria-lab-ui-6b4874b5d-6kr74 0/1 Running 0 34s pod/lab-odpi-egeria-lab-uistatic-6694896d7b-7qj2w 1/1 Running 0 34s pod/lab-zookeeper-0 1/1 Running 0 34s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/lab-core ClusterIP 172.21.163.188 <none> 9443/TCP 34s service/lab-datalake ClusterIP 172.21.25.50 <none> 9443/TCP 34s service/lab-dev ClusterIP 172.21.19.97 <none> 9443/TCP 34s service/lab-jupyter ClusterIP 172.21.200.189 <none> 8888/TCP 34s service/lab-kafka ClusterIP 172.21.199.61 <none> 9092/TCP 34s service/lab-kafka-headless ClusterIP None <none> 9092/TCP,9093/TCP 34s service/lab-nginx ClusterIP 172.21.245.104 <none> 443/TCP 34s service/lab-odpi-egeria-lab-factory ClusterIP 172.21.112.28 <none> 9443/TCP 34s service/lab-presentation ClusterIP 172.21.151.140 <none> 8091/TCP 34s service/lab-ui ClusterIP 172.21.115.64 <none> 8443/TCP 34s service/lab-uistatic ClusterIP 172.21.247.192 <none> 80/TCP 34s service/lab-zookeeper ClusterIP 172.21.100.132 <none> 2181/TCP,2888/TCP,3888/TCP 34s service/lab-zookeeper-headless ClusterIP None <none> 2181/TCP,2888/TCP,3888/TCP 34s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/lab-odpi-egeria-lab-jupyter 1/1 1 1 34s deployment.apps/lab-odpi-egeria-lab-nginx 1/1 1 1 34s deployment.apps/lab-odpi-egeria-lab-presentation 1/1 1 1 34s deployment.apps/lab-odpi-egeria-lab-ui 0/1 1 0 34s deployment.apps/lab-odpi-egeria-lab-uistatic 1/1 1 1 34s NAME DESIRED CURRENT READY AGE replicaset.apps/lab-odpi-egeria-lab-jupyter-bb7cf5f47 1 1 1 34s replicaset.apps/lab-odpi-egeria-lab-nginx-69f7cfc67b 1 1 1 34s replicaset.apps/lab-odpi-egeria-lab-presentation-66574b9796 1 1 1 34s replicaset.apps/lab-odpi-egeria-lab-ui-6b4874b5d 1 1 0 34s replicaset.apps/lab-odpi-egeria-lab-uistatic-6694896d7b 1 1 1 34s NAME READY AGE statefulset.apps/lab-kafka 0/1 34s statefulset.apps/lab-odpi-egeria-lab-core 0/1 34s statefulset.apps/lab-odpi-egeria-lab-datalake 0/1 34s statefulset.apps/lab-odpi-egeria-lab-dev 0/1 34s statefulset.apps/lab-odpi-egeria-lab-factory 0/1 34s statefulset.apps/lab-zookeeper 1/1 34s``` (Note that all of the `pod/...` listed at the top have `Running` as their `STATUS` and `1/1` under `READY`.) At this point you should be able to access your notebook by going to the port listed to the right of `service/lab-jupyter` -- by default (and in the case above) the port is `30888`: For example, if running Kubernetes locally on your machine, you should be able to get to the notebook with: ```text http://localhost:30888 If you are using a cloud service, you will need to know what external ip address or name is exposed. This may be called 'Ingress Domain' or similar, but will typically not be seen in the output above. For example: http://mycluster.mydomain.mycloud.com:30888 You may wish to expose these cluster IPs via a LoadBalancer. You can find an example of this in the egeria-base chart README. You will find the egeria open metadata lab notebooks already populated in the notebook server. Starting over \u00b6 Because the environment is entirely self-contained, you can easily start over the labs simply by deleting the deployment and running the installation again. This will wipe out all of the metadata across the lab Egeria servers, remove all messages from the Kafka bus used in the cohort, reset the Jupyter notebooks to their original clean state, etc. To delete the deployment, simply run this: $ helm delete lab Where lab is the name you used in your original deployment. (You can see what it's called by first running helm list and reviewing the output.) (Then just re-run the last command in the Installation section above to get a fresh environment.) Port Clashes \u00b6 The chart is configured to use a fixed set of ports and expose them using a 'NodePort' service as described above. You may find you clash with other services setup in your cluster. If so you can override the ports by creating a file such as lab.yaml with the following contents: service: type: NodePort nodeport: jupyter: 30888 core: 30080 datalake: 30081 dev: 30082 factory: 30083 ui: 30443 and then change the port numbers accordingly. Refer to the existing values file for additional ports in this section that may reflect new components as added You can then deploy using helm install lab odpi-egeria-lab -f lab.yaml which will override standard defaults with your choices Enabling persistence \u00b6 Support has been added to use persistence in these charts. See 'values.yaml' for more information on this option. You may also wish to refer to the 'egeria-base' helm chart which is a deployment of a single, persistent, autostart server with UI. Note however that since this will save the state of your configuration done from the tutorial notebooks it may be confusing - as such this is disabled by default. Using the environment to extend notebooks or develop new ones \u00b6 If you are using a notebook written to assume 'localhost:9443' or similar, replace with the following fragment. This will use the correct defaults for the environment (k8s or compose), or localhost if these are not yet. : corePlatformURL = os.environ.get('corePlatformURL','https://localhost:9443') dataLakePlatformURL = os.environ.get('dataLakePlatformURL','https://localhost:9444') devPlatformURL = os.environ.get('devPlatformURL','https://localhost:9445') factoryPlatformURL = os.environ.get('factoryPlatformURL','https://localhost:9446') License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Lab"},{"location":"charts/lab/#lab-coco-pharmaceuticals-odpi-egeria-lab","text":"This Helm chart will deploy the following components, all in a self-contained environment, for use in the Egeria hands-on labs -- allowing you to explore Egeria and its concepts safely and repeatably: Multiple Egeria servers Apache Kafka (and its Zookeeper dependency) Jupyter Notebooks","title":"Lab - Coco Pharmaceuticals (odpi-egeria-lab)"},{"location":"charts/lab/#prerequisites","text":"In order to use the labs, you'll first need to have the following installed: Kubernetes 1.15 or above Helm 3.0 or above The minimum tested configurations for Kubernetes are - Cloud/remote service - 3 nodes, 2GB ram per node - Local docker for mac/windows - 1 node, 6Gb ram dedicated to docker You could use the Docker-embedded Kubernetes for this on eg. Docker Desktop, or a public cloud service that provides Kubernetes If you need to install helm3, please obtain from https://github.com/helm/helm/releases before starting and ensure the 'helm' executable is in your PATH. The instructions and examples that follow assume use of this version If you can't meet these requirements, or would like to start with a simpler approach, an alternative environment for running the tutorials has been implemented using docker-compose & can be found at https://github.com/odpi/egeria/tree/master/open-metadata-resources/open-metadata-deployment/compose/tutorials .","title":"Prerequisites"},{"location":"charts/lab/#installation","text":"From one directory level above the location of this README, run the following: helm dep update odpi-egeria-lab helm install lab odpi-egeria-lab Example transcript: $ export PATH=~/bin:$PATH $ pwd /home/jonesn/egeria/open-metadata-resources/open-metadata-deployment/charts $ helm version version.BuildInfo{Version:\"v3.5.3\", GitCommit:\"041ce5a2c17a58be0fcd5f5e16fb3e7e95fea622\", GitTreeState:\"dirty\", GoVersion:\"go1.16\"} $ helm dep update odpi-egeria-lab Getting updates for unmanaged Helm repositories... ...Successfully got an update from the \"https://charts.bitnami.com/bitnami\" chart repository Saving 1 charts Downloading kafka from repo https://charts.bitnami.com/bitnami Deleting outdated charts Now we can actually do the deployment with helm install lab odpi-egeria-lab which will look like: $ helm install lab odpi-egeria-lab [16:52:10] NAME: lab LAST DEPLOYED: Tue Apr 6 16:52:17 2021 NAMESPACE: egeria-release STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ODPi Egeria lab tutorial --- The Egeria tutorials have now been deployed to Kubernetes. It may take a minute or so for everything to start up. Open your web browser and go to addressofmycluster:30888 to get started You may need to contact your cluster admin, or read your cloud provider helptext to understand the correct 'addressofmycluster' to use. If you experience problems, check memory consumption on your nodes. A minimum of a 3 node cluster, 2GB per node; or a desktop environment with 6GB dedicated is recommended. Please provide any feeback via a github issue at https://github.com/odpi/egeria or join us on slack via https://http://slack.lfai.foundation - The ODPi Egeria team Note that it can take a few seconds for the various components to all spin-up. You can monitor the readiness by running kubectl get all -- when ready, you should see output like the following: $ kubectl get all [16:52:24] NAME READY STATUS RESTARTS AGE pod/lab-odpi-egeria-lab-core-0 0/1 Running 0 34s pod/lab-odpi-egeria-lab-datalake-0 0/1 Running 0 34s pod/lab-odpi-egeria-lab-dev-0 0/1 Running 0 34s pod/lab-odpi-egeria-lab-factory-0 0/1 Running 0 34s pod/lab-odpi-egeria-lab-jupyter-bb7cf5f47-k6w2p 1/1 Running 0 34s pod/lab-odpi-egeria-lab-nginx-69f7cfc67b-nptz5 1/1 Running 0 34s pod/lab-odpi-egeria-lab-presentation-66574b9796-7t4rc 1/1 Running 0 34s pod/lab-odpi-egeria-lab-ui-6b4874b5d-6kr74 0/1 Running 0 34s pod/lab-odpi-egeria-lab-uistatic-6694896d7b-7qj2w 1/1 Running 0 34s pod/lab-zookeeper-0 1/1 Running 0 34s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/lab-core ClusterIP 172.21.163.188 <none> 9443/TCP 34s service/lab-datalake ClusterIP 172.21.25.50 <none> 9443/TCP 34s service/lab-dev ClusterIP 172.21.19.97 <none> 9443/TCP 34s service/lab-jupyter ClusterIP 172.21.200.189 <none> 8888/TCP 34s service/lab-kafka ClusterIP 172.21.199.61 <none> 9092/TCP 34s service/lab-kafka-headless ClusterIP None <none> 9092/TCP,9093/TCP 34s service/lab-nginx ClusterIP 172.21.245.104 <none> 443/TCP 34s service/lab-odpi-egeria-lab-factory ClusterIP 172.21.112.28 <none> 9443/TCP 34s service/lab-presentation ClusterIP 172.21.151.140 <none> 8091/TCP 34s service/lab-ui ClusterIP 172.21.115.64 <none> 8443/TCP 34s service/lab-uistatic ClusterIP 172.21.247.192 <none> 80/TCP 34s service/lab-zookeeper ClusterIP 172.21.100.132 <none> 2181/TCP,2888/TCP,3888/TCP 34s service/lab-zookeeper-headless ClusterIP None <none> 2181/TCP,2888/TCP,3888/TCP 34s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/lab-odpi-egeria-lab-jupyter 1/1 1 1 34s deployment.apps/lab-odpi-egeria-lab-nginx 1/1 1 1 34s deployment.apps/lab-odpi-egeria-lab-presentation 1/1 1 1 34s deployment.apps/lab-odpi-egeria-lab-ui 0/1 1 0 34s deployment.apps/lab-odpi-egeria-lab-uistatic 1/1 1 1 34s NAME DESIRED CURRENT READY AGE replicaset.apps/lab-odpi-egeria-lab-jupyter-bb7cf5f47 1 1 1 34s replicaset.apps/lab-odpi-egeria-lab-nginx-69f7cfc67b 1 1 1 34s replicaset.apps/lab-odpi-egeria-lab-presentation-66574b9796 1 1 1 34s replicaset.apps/lab-odpi-egeria-lab-ui-6b4874b5d 1 1 0 34s replicaset.apps/lab-odpi-egeria-lab-uistatic-6694896d7b 1 1 1 34s NAME READY AGE statefulset.apps/lab-kafka 0/1 34s statefulset.apps/lab-odpi-egeria-lab-core 0/1 34s statefulset.apps/lab-odpi-egeria-lab-datalake 0/1 34s statefulset.apps/lab-odpi-egeria-lab-dev 0/1 34s statefulset.apps/lab-odpi-egeria-lab-factory 0/1 34s statefulset.apps/lab-zookeeper 1/1 34s``` (Note that all of the `pod/...` listed at the top have `Running` as their `STATUS` and `1/1` under `READY`.) At this point you should be able to access your notebook by going to the port listed to the right of `service/lab-jupyter` -- by default (and in the case above) the port is `30888`: For example, if running Kubernetes locally on your machine, you should be able to get to the notebook with: ```text http://localhost:30888 If you are using a cloud service, you will need to know what external ip address or name is exposed. This may be called 'Ingress Domain' or similar, but will typically not be seen in the output above. For example: http://mycluster.mydomain.mycloud.com:30888 You may wish to expose these cluster IPs via a LoadBalancer. You can find an example of this in the egeria-base chart README. You will find the egeria open metadata lab notebooks already populated in the notebook server.","title":"Installation"},{"location":"charts/lab/#starting-over","text":"Because the environment is entirely self-contained, you can easily start over the labs simply by deleting the deployment and running the installation again. This will wipe out all of the metadata across the lab Egeria servers, remove all messages from the Kafka bus used in the cohort, reset the Jupyter notebooks to their original clean state, etc. To delete the deployment, simply run this: $ helm delete lab Where lab is the name you used in your original deployment. (You can see what it's called by first running helm list and reviewing the output.) (Then just re-run the last command in the Installation section above to get a fresh environment.)","title":"Starting over"},{"location":"charts/lab/#port-clashes","text":"The chart is configured to use a fixed set of ports and expose them using a 'NodePort' service as described above. You may find you clash with other services setup in your cluster. If so you can override the ports by creating a file such as lab.yaml with the following contents: service: type: NodePort nodeport: jupyter: 30888 core: 30080 datalake: 30081 dev: 30082 factory: 30083 ui: 30443 and then change the port numbers accordingly. Refer to the existing values file for additional ports in this section that may reflect new components as added You can then deploy using helm install lab odpi-egeria-lab -f lab.yaml which will override standard defaults with your choices","title":"Port Clashes"},{"location":"charts/lab/#enabling-persistence","text":"Support has been added to use persistence in these charts. See 'values.yaml' for more information on this option. You may also wish to refer to the 'egeria-base' helm chart which is a deployment of a single, persistent, autostart server with UI. Note however that since this will save the state of your configuration done from the tutorial notebooks it may be confusing - as such this is disabled by default.","title":"Enabling persistence"},{"location":"charts/lab/#using-the-environment-to-extend-notebooks-or-develop-new-ones","text":"If you are using a notebook written to assume 'localhost:9443' or similar, replace with the following fragment. This will use the correct defaults for the environment (k8s or compose), or localhost if these are not yet. : corePlatformURL = os.environ.get('corePlatformURL','https://localhost:9443') dataLakePlatformURL = os.environ.get('dataLakePlatformURL','https://localhost:9444') devPlatformURL = os.environ.get('devPlatformURL','https://localhost:9445') factoryPlatformURL = os.environ.get('factoryPlatformURL','https://localhost:9446') License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Using the environment to extend notebooks or develop new ones"},{"location":"contribute/devguide/","text":"to be written ... \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Devguide"},{"location":"contribute/devguide/#to-be-written","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"to be written ..."},{"location":"intro/helm/","text":"Helm \u00b6 Helm is the best way to find, share, and use software built for Kubernetes. - https://helm.sh Deploying apps in Kubernetes \u00b6 In Kubernetes, resources such as pods (to run code) or services (for network accessibility) are defined in yaml. One or more documents can be submitted at a time. So we might have one yaml file that defines our pod - with information about which container images to run, and another to setup a network service. finally we may have another that describes our storage requirements (a persistent volume claim ) What does Helm do? \u00b6 Helm provides a way of bunding yaml files together into an archive, together with a templating mechanism to allow reuse of common patterns & ensure different yamls are consistent and can reference each other. These archives are known as 'Charts' and can be hosted in a known format as a 'chart repository'. The archives are versioned. Helm is basically focussed on creating yaml documents that are submitted to Kubernetes - it is not involved in the runtime of a Kubernetes environment. Once a helm app is installed, interaction is just with the regular kubernetes objects. Helm Commands include options to * install * uninstall * list * search * upgrade * rollback Installing Helm \u00b6 Some Kubernetes environments may install helm as part of their client tooling, refer to the docs to see if this is the case, and run helm version to check - expect to use v3 or above. If so, install can be skipped. MacOS \u00b6 If using macOS with HomeBrew installed, helm can be simply installed with brew install helm Other platforms (Linux, Windows) \u00b6 See the Installation Guide for more ways to install Helm Accessing the egeria charts repository \u00b6 Our helm charts for Egeria are stored in a repository hosted on GitHub. The source for these is at https://github.com/odpi/egeria-charts , and as charts are updated they are automatically published to a GitHub pages Website (in fact this one!) Run the following to add this repository helm repo add egeria https://odpi.github.io/egeria-charts Before searching or installing, always update your local copy of the repository helm repo update egeria You can now list released charts: helm search repo egeria or development charts (being worked on, or using code from master) helm search repo egeria --devel and install a chart that looks interesting - helm install egeria/<chart> License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Helm"},{"location":"intro/helm/#helm","text":"Helm is the best way to find, share, and use software built for Kubernetes. - https://helm.sh","title":"Helm"},{"location":"intro/helm/#deploying-apps-in-kubernetes","text":"In Kubernetes, resources such as pods (to run code) or services (for network accessibility) are defined in yaml. One or more documents can be submitted at a time. So we might have one yaml file that defines our pod - with information about which container images to run, and another to setup a network service. finally we may have another that describes our storage requirements (a persistent volume claim )","title":"Deploying apps in Kubernetes"},{"location":"intro/helm/#what-does-helm-do","text":"Helm provides a way of bunding yaml files together into an archive, together with a templating mechanism to allow reuse of common patterns & ensure different yamls are consistent and can reference each other. These archives are known as 'Charts' and can be hosted in a known format as a 'chart repository'. The archives are versioned. Helm is basically focussed on creating yaml documents that are submitted to Kubernetes - it is not involved in the runtime of a Kubernetes environment. Once a helm app is installed, interaction is just with the regular kubernetes objects. Helm Commands include options to * install * uninstall * list * search * upgrade * rollback","title":"What does Helm do?"},{"location":"intro/helm/#installing-helm","text":"Some Kubernetes environments may install helm as part of their client tooling, refer to the docs to see if this is the case, and run helm version to check - expect to use v3 or above. If so, install can be skipped.","title":"Installing Helm"},{"location":"intro/helm/#macos","text":"If using macOS with HomeBrew installed, helm can be simply installed with brew install helm","title":"MacOS"},{"location":"intro/helm/#other-platforms-linux-windows","text":"See the Installation Guide for more ways to install Helm","title":"Other platforms (Linux, Windows)"},{"location":"intro/helm/#accessing-the-egeria-charts-repository","text":"Our helm charts for Egeria are stored in a repository hosted on GitHub. The source for these is at https://github.com/odpi/egeria-charts , and as charts are updated they are automatically published to a GitHub pages Website (in fact this one!) Run the following to add this repository helm repo add egeria https://odpi.github.io/egeria-charts Before searching or installing, always update your local copy of the repository helm repo update egeria You can now list released charts: helm search repo egeria or development charts (being worked on, or using code from master) helm search repo egeria --devel and install a chart that looks interesting - helm install egeria/<chart> License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing the egeria charts repository"},{"location":"intro/k8s/","text":"What is Kubernetes? \u00b6 Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. - https://kubernetes.io This is how the official website describes it. It's effectively a standardized way of deploying applications in a very scalable way - from everything such as development prototyping through to massive highly available enterprise solutions. In this document I'll give a very brief summary that should help those of you new to Kubernetes to make your first steps with Egeria. What are the key concepts in Kubernetes? \u00b6 These are just some of the concepts that can help to understand what's going on. This isn't a complete list. Api \u00b6 Kubernetes using a standard API which is oriented around manipulating Objects. The commands are therefore very standard, it's all about the objects. Making it so \u00b6 The system is always observing the state of the system through these objects, and where there are discrepancies, taking action to 'Make it So' as Captain Picard would say. The approach is imperitive. So we think of these objects as describing the desired state of the system. Namespace \u00b6 A namespace provides a way of seperating out kubernetes resources by users or applications as a convenience. It keeps names more understandable, and avoids global duplicates. For example a developer working on a k8s cluster may have a namespace of their own to experiment in. Container \u00b6 A Container is what runs stuff. It's similar to a Virtual Machine in some ways, but much more lightweight. Containers use code Images which may be custom built, or very standard off-the-shelf reusable items. Containers are typically very focussed on a single need or application. Pod \u00b6 A Pod is a single group of one or more containers. Typically a single main container runs in a pod, but this may be supported by additional containers for log, audit, security, initialization etc. Think of this as an atomic unit that can run a workload. Pods are disposeable - they will come and go. Other objects are concerned with providing a reliable service. Service \u00b6 A service provides network accessibility to one or more pods. The service name will be added into local Domain Name Service (DNS) for easy accessibility from other pods. Load can be shared across multiple pods Ingres \u00b6 Think of Ingress as the entry point to Kubernetes services from an external network perspective - so it is these addresses external users would be aware of. Deployment \u00b6 A deployment keeps a set of pods running - including replica copies, ie restarted if stopped, matching resource requirements, handling node failure . Stateful Set \u00b6 A stateful set goes further than a deployment in that it keeps a well known identifier for each identical replica. This helps in allocating persistent storage & network resources to a replica ConfigMap \u00b6 A config map is a way of keeping configuration (exposed as files or environment variables) seperate to an application. Secret \u00b6 A secret is used to keep information secret, as the name might suggest ... This might be a password or an API key & the data is encrypted Custom Objects \u00b6 In addition to this list -- and many more covered in the official documentation -- Kubernetes also supports custom resources. These form a key part of Kubernetes Operators . Storage \u00b6 Pods can request storage - which is known as a persistent volume claim (PVC), which are either manually or automatically resolved to a persistent volume. See the k8s docs Persistent Volumes Why are we using Kubernetes? \u00b6 All sizes of systems can run kubernetes applications - from a small raspberry pi through desktops and workstations through to huge cloud deployments. Whilst the details around storage, security, networking etc do vary by implementation, the core concepts, and configurations work across all. Some may be more concerned about an easy way to play with development code, try out new ideas, whilst at the far end of the spectrum enterprises want something super scalable and reliable, and easy to monitor. For egeria we want to achieve two main things * Provide easy to use demos and tutorials that show how Egeria can be used and worked with without requiring too much complex setup. * Provide examples that show how Egeria can be deployed in k8s, and then adapted for the organization's needs. Other alternatives that might come to mind include * Docker -- whilst simple, this is more geared around running a single container, and building complex environment means a lot of work combining application stacks together, often resulting in something that isn't usable. We do of course still have container images, which are essential to k8s, but these are simple & self contained. * docker-compose -- this builds on docker in allowing multiple containers and servers to be orchestrated, but it's much less flexible & scalable than kubernetes. How do I get access to Kubernetes? \u00b6 Getting Started provides links to setting up Kubernetes in many environments. Below we'll take a quick look at some of the simpler examples, especially for new users. microk8s (Linux, Windows, macOS) \u00b6 Official microk8s site 4GB is recommended as a minimum memory requirement. As with most k8s implementations, when running some ongoing cpu will be used, so if running on your laptop/low power device it's recommended to refer to the relevant docs & stop k8s when not in use. When running on a separate server or a cloud service this isn't a concern. MacOS \u00b6 The macos install docs cover the steps needed to install microk8s. Most of the Egeria development team use MacOS, so the instructions are elaborated and qualified here: The recommended approach uses HomeBrew . This offers a suite of tools often found on linux which are easy to setup on macOS. See install docs IMPORTANT: Before installing, go into System Preferences->Security & Privacy. Click the lock to get into Admin mode. Then ensure Firewall Options->Enable Stealth Mode is NOT enabled (no tick). If it is, microk8s will not work properly . More If you are concerned over the firewall change, or homebrew requirement, refer back to the official k8s documentation & choose another k8s implementation that works for you. Ensure you turn on the following services: storage, dns, helm3 . dashboard is also useful to understand more about k8s and what is running. However it is currently failing as described in issue 2507 As an example, the following commands should get you set up, but always check the official docs for current details brew install ubuntu/microk8s/microk8s microk8s install microk8s status --wait-ready microk8s enable dns storage helm3 microk8s kubectl get all --all-namespaces Kubernetes is now running. Windows \u00b6 Follow the official instructions (untested) Linux \u00b6 Follow the official instructions (untested) Docker Desktop (Windows, macOS) \u00b6 Docker Desktop supports Kubernetes After installing, go into Docker Desktop 'settings and select 'Kubernetes'. Make sure 'Enable Kubernetes' is checked. Also under resources ensure at least 4GB is allocated to Docker Cloud \u00b6 Many cloud providers offer Kubernetes deployments which can be used for experimentation or production. This include Redhat OpenShift on multiple cloud providers including on IBMCloud Kubernetes on IBMCloud Azure Kubernetes Service Google Kubernetes Engine (GKE) In addition to a cloud install, ensure you have installed the relevant cloud provider's tooling to manage their k8s environment, including having access to the standard kubernetes command kubectl . Note that in the team's testing we mostly are running Redhat OpenShift on IBMCloud as a managed service. We welcome feedback of running our examples on other environments, especially as some of the specifics around ingress rules, storage, security can vary. \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"K8s"},{"location":"intro/k8s/#what-is-kubernetes","text":"Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. - https://kubernetes.io This is how the official website describes it. It's effectively a standardized way of deploying applications in a very scalable way - from everything such as development prototyping through to massive highly available enterprise solutions. In this document I'll give a very brief summary that should help those of you new to Kubernetes to make your first steps with Egeria.","title":"What is Kubernetes?"},{"location":"intro/k8s/#what-are-the-key-concepts-in-kubernetes","text":"These are just some of the concepts that can help to understand what's going on. This isn't a complete list.","title":"What are the key concepts in Kubernetes?"},{"location":"intro/k8s/#api","text":"Kubernetes using a standard API which is oriented around manipulating Objects. The commands are therefore very standard, it's all about the objects.","title":"Api"},{"location":"intro/k8s/#making-it-so","text":"The system is always observing the state of the system through these objects, and where there are discrepancies, taking action to 'Make it So' as Captain Picard would say. The approach is imperitive. So we think of these objects as describing the desired state of the system.","title":"Making it so"},{"location":"intro/k8s/#namespace","text":"A namespace provides a way of seperating out kubernetes resources by users or applications as a convenience. It keeps names more understandable, and avoids global duplicates. For example a developer working on a k8s cluster may have a namespace of their own to experiment in.","title":"Namespace"},{"location":"intro/k8s/#container","text":"A Container is what runs stuff. It's similar to a Virtual Machine in some ways, but much more lightweight. Containers use code Images which may be custom built, or very standard off-the-shelf reusable items. Containers are typically very focussed on a single need or application.","title":"Container"},{"location":"intro/k8s/#pod","text":"A Pod is a single group of one or more containers. Typically a single main container runs in a pod, but this may be supported by additional containers for log, audit, security, initialization etc. Think of this as an atomic unit that can run a workload. Pods are disposeable - they will come and go. Other objects are concerned with providing a reliable service.","title":"Pod"},{"location":"intro/k8s/#service","text":"A service provides network accessibility to one or more pods. The service name will be added into local Domain Name Service (DNS) for easy accessibility from other pods. Load can be shared across multiple pods","title":"Service"},{"location":"intro/k8s/#ingres","text":"Think of Ingress as the entry point to Kubernetes services from an external network perspective - so it is these addresses external users would be aware of.","title":"Ingres"},{"location":"intro/k8s/#deployment","text":"A deployment keeps a set of pods running - including replica copies, ie restarted if stopped, matching resource requirements, handling node failure .","title":"Deployment"},{"location":"intro/k8s/#stateful-set","text":"A stateful set goes further than a deployment in that it keeps a well known identifier for each identical replica. This helps in allocating persistent storage & network resources to a replica","title":"Stateful Set"},{"location":"intro/k8s/#configmap","text":"A config map is a way of keeping configuration (exposed as files or environment variables) seperate to an application.","title":"ConfigMap"},{"location":"intro/k8s/#secret","text":"A secret is used to keep information secret, as the name might suggest ... This might be a password or an API key & the data is encrypted","title":"Secret"},{"location":"intro/k8s/#custom-objects","text":"In addition to this list -- and many more covered in the official documentation -- Kubernetes also supports custom resources. These form a key part of Kubernetes Operators .","title":"Custom Objects"},{"location":"intro/k8s/#storage","text":"Pods can request storage - which is known as a persistent volume claim (PVC), which are either manually or automatically resolved to a persistent volume. See the k8s docs Persistent Volumes","title":"Storage"},{"location":"intro/k8s/#why-are-we-using-kubernetes","text":"All sizes of systems can run kubernetes applications - from a small raspberry pi through desktops and workstations through to huge cloud deployments. Whilst the details around storage, security, networking etc do vary by implementation, the core concepts, and configurations work across all. Some may be more concerned about an easy way to play with development code, try out new ideas, whilst at the far end of the spectrum enterprises want something super scalable and reliable, and easy to monitor. For egeria we want to achieve two main things * Provide easy to use demos and tutorials that show how Egeria can be used and worked with without requiring too much complex setup. * Provide examples that show how Egeria can be deployed in k8s, and then adapted for the organization's needs. Other alternatives that might come to mind include * Docker -- whilst simple, this is more geared around running a single container, and building complex environment means a lot of work combining application stacks together, often resulting in something that isn't usable. We do of course still have container images, which are essential to k8s, but these are simple & self contained. * docker-compose -- this builds on docker in allowing multiple containers and servers to be orchestrated, but it's much less flexible & scalable than kubernetes.","title":"Why are we using Kubernetes?"},{"location":"intro/k8s/#how-do-i-get-access-to-kubernetes","text":"Getting Started provides links to setting up Kubernetes in many environments. Below we'll take a quick look at some of the simpler examples, especially for new users.","title":"How do I get access to Kubernetes?"},{"location":"intro/k8s/#microk8s-linux-windows-macos","text":"Official microk8s site 4GB is recommended as a minimum memory requirement. As with most k8s implementations, when running some ongoing cpu will be used, so if running on your laptop/low power device it's recommended to refer to the relevant docs & stop k8s when not in use. When running on a separate server or a cloud service this isn't a concern.","title":"microk8s (Linux, Windows, macOS)"},{"location":"intro/k8s/#macos","text":"The macos install docs cover the steps needed to install microk8s. Most of the Egeria development team use MacOS, so the instructions are elaborated and qualified here: The recommended approach uses HomeBrew . This offers a suite of tools often found on linux which are easy to setup on macOS. See install docs IMPORTANT: Before installing, go into System Preferences->Security & Privacy. Click the lock to get into Admin mode. Then ensure Firewall Options->Enable Stealth Mode is NOT enabled (no tick). If it is, microk8s will not work properly . More If you are concerned over the firewall change, or homebrew requirement, refer back to the official k8s documentation & choose another k8s implementation that works for you. Ensure you turn on the following services: storage, dns, helm3 . dashboard is also useful to understand more about k8s and what is running. However it is currently failing as described in issue 2507 As an example, the following commands should get you set up, but always check the official docs for current details brew install ubuntu/microk8s/microk8s microk8s install microk8s status --wait-ready microk8s enable dns storage helm3 microk8s kubectl get all --all-namespaces Kubernetes is now running.","title":"MacOS"},{"location":"intro/k8s/#windows","text":"Follow the official instructions (untested)","title":"Windows"},{"location":"intro/k8s/#linux","text":"Follow the official instructions (untested)","title":"Linux"},{"location":"intro/k8s/#docker-desktop-windows-macos","text":"Docker Desktop supports Kubernetes After installing, go into Docker Desktop 'settings and select 'Kubernetes'. Make sure 'Enable Kubernetes' is checked. Also under resources ensure at least 4GB is allocated to Docker","title":"Docker Desktop (Windows, macOS)"},{"location":"intro/k8s/#cloud","text":"Many cloud providers offer Kubernetes deployments which can be used for experimentation or production. This include Redhat OpenShift on multiple cloud providers including on IBMCloud Kubernetes on IBMCloud Azure Kubernetes Service Google Kubernetes Engine (GKE) In addition to a cloud install, ensure you have installed the relevant cloud provider's tooling to manage their k8s environment, including having access to the standard kubernetes command kubectl .","title":"Cloud"},{"location":"intro/k8s/#note-that-in-the-teams-testing-we-mostly-are-running-redhat-openshift-on-ibmcloud-as-a-managed-service-we-welcome-feedback-of-running-our-examples-on-other-environments-especially-as-some-of-the-specifics-around-ingress-rules-storage-security-can-vary","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Note that in the team's testing we mostly are running Redhat OpenShift on IBMCloud as a managed service. We welcome feedback of running our examples on other environments, especially as some of the specifics around ingress rules, storage, security can vary."},{"location":"more/operator/","text":"Operator - to be written \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Operator"},{"location":"more/operator/#operator-to-be-written","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Operator - to be written"}]}